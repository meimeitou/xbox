+++
title = 'çº¿æ€§å›å½’'
math = true
+++

çº¿æ€§å›å½’ï¼šç”¨çº¿æ€§å‡½æ•°é¢„æµ‹ç”Ÿæ´»

- [å¼•è¨€](#å¼•è¨€)
- [ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’ï¼Ÿ](#ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’)
  - [æ•°å­¦è¡¨è¾¾å¼](#æ•°å­¦è¡¨è¾¾å¼)
  - [ç”Ÿæ´»ä¸­çš„çº¿æ€§å…³ç³»](#ç”Ÿæ´»ä¸­çš„çº¿æ€§å…³ç³»)
- [ä»æ•£ç‚¹å›¾åˆ°ç›´çº¿ï¼šç›´è§‚ç†è§£](#ä»æ•£ç‚¹å›¾åˆ°ç›´çº¿ç›´è§‚ç†è§£)
- [çº¿æ€§å›å½’çš„æ ¸å¿ƒæ€æƒ³](#çº¿æ€§å›å½’çš„æ ¸å¿ƒæ€æƒ³)
  - [1. æœ€ä½³æ‹Ÿåˆç›´çº¿](#1-æœ€ä½³æ‹Ÿåˆç›´çº¿)
  - [2. æœ€å°äºŒä¹˜æ³•](#2-æœ€å°äºŒä¹˜æ³•)
- [å¤šå…ƒçº¿æ€§å›å½’ï¼šæ›´å¤æ‚çš„ç°å®](#å¤šå…ƒçº¿æ€§å›å½’æ›´å¤æ‚çš„ç°å®)
- [æ‰‹åŠ¨å®ç°çº¿æ€§å›å½’](#æ‰‹åŠ¨å®ç°çº¿æ€§å›å½’)
- [æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£çº¿æ€§å›å½’](#æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£çº¿æ€§å›å½’)
- [æ¨¡å‹è¯„ä¼°ä¸è¯Šæ–­](#æ¨¡å‹è¯„ä¼°ä¸è¯Šæ–­)
  - [1. è¯„ä¼°æŒ‡æ ‡](#1-è¯„ä¼°æŒ‡æ ‡)
  - [2. è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–](#2-è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–)
- [å®é™…åº”ç”¨æ¡ˆä¾‹](#å®é™…åº”ç”¨æ¡ˆä¾‹)
  - [æˆ¿ä»·é¢„æµ‹å®Œæ•´æ¡ˆä¾‹](#æˆ¿ä»·é¢„æµ‹å®Œæ•´æ¡ˆä¾‹)
- [çº¿æ€§å›å½’çš„å‡è®¾ä¸å±€é™æ€§](#çº¿æ€§å›å½’çš„å‡è®¾ä¸å±€é™æ€§)
  - [åŸºæœ¬å‡è®¾](#åŸºæœ¬å‡è®¾)
  - [å‡è®¾æ£€éªŒ](#å‡è®¾æ£€éªŒ)
- [æ€»ç»“ï¼šçº¿æ€§å›å½’çš„æ ¸å¿ƒè¦ç‚¹](#æ€»ç»“çº¿æ€§å›å½’çš„æ ¸å¿ƒè¦ç‚¹)
  - [ğŸ¯ æ ¸å¿ƒæ€æƒ³](#-æ ¸å¿ƒæ€æƒ³)
  - [ğŸ“Š å…³é”®æ¦‚å¿µ](#-å…³é”®æ¦‚å¿µ)
  - [ğŸ’¡ å®ç”¨æŠ€å·§](#-å®ç”¨æŠ€å·§)
  - [ğŸª åº”ç”¨åœºæ™¯](#-åº”ç”¨åœºæ™¯)
  - [ğŸ”§ è®°å¿†å£è¯€](#-è®°å¿†å£è¯€)

## å¼•è¨€

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨ä¹°æˆ¿ï¼Œæˆ¿äº§ç»çºªäººå‘Šè¯‰ä½ ï¼š"è¿™ä¸ªåŒºåŸŸï¼Œæˆ¿å­æ¯å¤§10å¹³æ–¹ç±³ï¼Œä»·æ ¼å¤§çº¦å¤š10ä¸‡å…ƒã€‚"è¿™ä¸ªç®€å•çš„æè¿°ï¼Œå…¶å®å°±åŒ…å«äº†çº¿æ€§å›å½’çš„æ ¸å¿ƒæ€æƒ³ï¼

çº¿æ€§å›å½’æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€åŸºç¡€ã€æœ€é‡è¦çš„ç®—æ³•ä¹‹ä¸€ã€‚å®ƒç®€å•æ˜“æ‡‚ï¼Œå´å¨åŠ›æ— ç©·ï¼Œæ˜¯é€šå¾€å¤æ‚ç®—æ³•çš„å¿…ç»ä¹‹è·¯ã€‚

## ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’ï¼Ÿ

çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰æ˜¯ä¸€ç§**é¢„æµ‹**æ–¹æ³•ï¼Œå®ƒå‡è®¾ç›®æ ‡å˜é‡ä¸è¾“å…¥ç‰¹å¾ä¹‹é—´å­˜åœ¨**çº¿æ€§å…³ç³»**ã€‚

### æ•°å­¦è¡¨è¾¾å¼

æœ€ç®€å•çš„çº¿æ€§å›å½’æ–¹ç¨‹ï¼š

```
y = wx + b
```

å…¶ä¸­ï¼š

- `y`ï¼šæˆ‘ä»¬è¦é¢„æµ‹çš„ç›®æ ‡ï¼ˆæˆ¿ä»·ã€æˆç»©ã€é”€é‡ç­‰ï¼‰
- `x`ï¼šè¾“å…¥ç‰¹å¾ï¼ˆé¢ç§¯ã€å­¦ä¹ æ—¶é—´ã€å¹¿å‘Šè´¹ç”¨ç­‰ï¼‰
- `w`ï¼šæƒé‡/æ–œç‡ï¼ˆæ¯å•ä½xå¯¹yçš„å½±å“ï¼‰
- `b`ï¼šåç½®/æˆªè·ï¼ˆåŸºç¡€å€¼ï¼‰

### ç”Ÿæ´»ä¸­çš„çº¿æ€§å…³ç³»

1. **æˆ¿ä»·ä¸é¢ç§¯** ğŸ 

   ```
   æˆ¿ä»· = å•ä»· Ã— é¢ç§¯ + åŸºç¡€è´¹ç”¨
   ```

2. **è€ƒè¯•æˆç»©ä¸å­¦ä¹ æ—¶é—´** ğŸ“š

   ```
   æˆç»© = æ•ˆç‡ Ã— å­¦ä¹ æ—¶é—´ + åŸºç¡€æ°´å¹³
   ```

3. **å·¥èµ„ä¸å·¥ä½œå¹´é™** ğŸ’¼

   ```
   å·¥èµ„ = å¹´å¢é•¿ç‡ Ã— å·¥ä½œå¹´é™ + èµ·è–ª
   ```

4. **æ±½è½¦æ²¹è€—ä¸é€Ÿåº¦** ğŸš—

   ```
   æ²¹è€— = é€Ÿåº¦ç³»æ•° Ã— é€Ÿåº¦ + åŸºç¡€æ²¹è€—
   ```

## ä»æ•£ç‚¹å›¾åˆ°ç›´çº¿ï¼šç›´è§‚ç†è§£

è®©æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ä¾‹å­å¼€å§‹ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import pandas as pd

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼šæˆ¿å±‹é¢ç§¯ä¸ä»·æ ¼
np.random.seed(42)
area = np.random.uniform(50, 200, 100)  # é¢ç§¯ï¼š50-200å¹³æ–¹ç±³
noise = np.random.normal(0, 10, 100)    # éšæœºå™ªå£°
price = 2 * area + 50 + noise           # ä»·æ ¼ = 2ä¸‡/å¹³ç±³ Ã— é¢ç§¯ + 50ä¸‡åŸºç¡€ + å™ªå£°

# å¯è§†åŒ–æ•°æ®
plt.figure(figsize=(12, 8))

# å­å›¾1ï¼šæ•£ç‚¹å›¾
plt.subplot(2, 2, 1)
plt.scatter(area, price, alpha=0.6, color='blue')
plt.xlabel('æˆ¿å±‹é¢ç§¯ (å¹³æ–¹ç±³)')
plt.ylabel('æˆ¿ä»· (ä¸‡å…ƒ)')
plt.title('æˆ¿ä»·ä¸é¢ç§¯çš„å…³ç³»ï¼ˆæ•£ç‚¹å›¾ï¼‰')
plt.grid(True, alpha=0.3)

# å­å›¾2ï¼šæ·»åŠ æ‹Ÿåˆç›´çº¿
plt.subplot(2, 2, 2)
plt.scatter(area, price, alpha=0.6, color='blue', label='å®é™…æ•°æ®')

# æ‹Ÿåˆçº¿æ€§å›å½’
model = LinearRegression()
area_2d = area.reshape(-1, 1)  # sklearnéœ€è¦2Dæ•°ç»„
model.fit(area_2d, price)

# é¢„æµ‹å¹¶ç»˜åˆ¶ç›´çº¿
area_line = np.linspace(50, 200, 100)
price_pred = model.predict(area_line.reshape(-1, 1))
plt.plot(area_line, price_pred, 'r-', linewidth=2, label=f'æ‹Ÿåˆç›´çº¿: y={model.coef_[0]:.2f}x+{model.intercept_:.2f}')

plt.xlabel('æˆ¿å±‹é¢ç§¯ (å¹³æ–¹ç±³)')
plt.ylabel('æˆ¿ä»· (ä¸‡å…ƒ)')
plt.title('çº¿æ€§å›å½’æ‹Ÿåˆç»“æœ')
plt.legend()
plt.grid(True, alpha=0.3)

# å­å›¾3ï¼šæ®‹å·®å›¾
plt.subplot(2, 2, 3)
predictions = model.predict(area_2d)
residuals = price - predictions
plt.scatter(area, residuals, alpha=0.6, color='green')
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('æˆ¿å±‹é¢ç§¯ (å¹³æ–¹ç±³)')
plt.ylabel('æ®‹å·® (å®é™…å€¼ - é¢„æµ‹å€¼)')
plt.title('æ®‹å·®åˆ†æ')
plt.grid(True, alpha=0.3)

# å­å›¾4ï¼šé¢„æµ‹æ•ˆæœ
plt.subplot(2, 2, 4)
plt.scatter(price, predictions, alpha=0.6)
plt.plot([price.min(), price.max()], [price.min(), price.max()], 'r--', linewidth=2)
plt.xlabel('å®é™…æˆ¿ä»· (ä¸‡å…ƒ)')
plt.ylabel('é¢„æµ‹æˆ¿ä»· (ä¸‡å…ƒ)')
plt.title('é¢„æµ‹ vs å®é™…')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"æ‹Ÿåˆç»“æœ:")
print(f"æ–œç‡(æ¯å¹³ç±³ä»·æ ¼): {model.coef_[0]:.2f} ä¸‡å…ƒ/å¹³ç±³")
print(f"æˆªè·(åŸºç¡€ä»·æ ¼): {model.intercept_:.2f} ä¸‡å…ƒ")
print(f"RÂ²åˆ†æ•°: {model.score(area_2d, price):.3f}")
```

## çº¿æ€§å›å½’çš„æ ¸å¿ƒæ€æƒ³

### 1. æœ€ä½³æ‹Ÿåˆç›´çº¿

çº¿æ€§å›å½’çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€æ¡**æœ€ä½³æ‹Ÿåˆç›´çº¿**ï¼Œä½¿å¾—ï¼š

- ç›´çº¿å°½å¯èƒ½æ¥è¿‘æ‰€æœ‰æ•°æ®ç‚¹
- é¢„æµ‹è¯¯å·®æœ€å°

### 2. æœ€å°äºŒä¹˜æ³•

æˆ‘ä»¬é€šè¿‡**æœ€å°åŒ–å¹³æ–¹è¯¯å·®**æ¥æ‰¾åˆ°æœ€ä½³ç›´çº¿ï¼š

```python
def visualize_least_squares():
    """å¯è§†åŒ–æœ€å°äºŒä¹˜æ³•çš„åŸç†"""
    
    # ç®€å•æ•°æ®
    x = np.array([1, 2, 3, 4, 5])
    y = np.array([2, 4, 5, 4, 5])
    
    # æ‹Ÿåˆç›´çº¿
    model = LinearRegression()
    x_2d = x.reshape(-1, 1)
    model.fit(x_2d, y)
    y_pred = model.predict(x_2d)
    
    plt.figure(figsize=(12, 5))
    
    # å·¦å›¾ï¼šæ˜¾ç¤ºè¯¯å·®
    plt.subplot(1, 2, 1)
    plt.scatter(x, y, color='blue', s=100, label='å®é™…æ•°æ®ç‚¹')
    plt.plot(x, y_pred, 'r-', linewidth=2, label='æ‹Ÿåˆç›´çº¿')
    
    # ç»˜åˆ¶è¯¯å·®çº¿
    for i in range(len(x)):
        plt.plot([x[i], x[i]], [y[i], y_pred[i]], 'g--', alpha=0.7)
        plt.text(x[i]+0.1, (y[i]+y_pred[i])/2, f'è¯¯å·®:{y[i]-y_pred[i]:.1f}', fontsize=8)
    
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('æœ€å°äºŒä¹˜æ³•ï¼šæœ€å°åŒ–è¯¯å·®å¹³æ–¹å’Œ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å³å›¾ï¼šä¸åŒç›´çº¿çš„è¯¯å·®å¯¹æ¯”
    plt.subplot(1, 2, 2)
    plt.scatter(x, y, color='blue', s=100, label='å®é™…æ•°æ®')
    
    # å‡ æ¡ä¸åŒçš„ç›´çº¿
    slopes = [0.5, 0.8, 1.0]  # ä¸åŒæ–œç‡
    colors = ['orange', 'green', 'red']
    
    for slope, color in zip(slopes, colors):
        y_line = slope * x + 1
        mse = np.mean((y - y_line)**2)
        plt.plot(x, y_line, color=color, label=f'æ–œç‡={slope}, MSE={mse:.2f}')
    
    # æœ€ä¼˜ç›´çº¿
    mse_best = np.mean((y - y_pred)**2)
    plt.plot(x, y_pred, 'purple', linewidth=3, 
             label=f'æœ€ä¼˜ç›´çº¿, MSE={mse_best:.2f}')
    
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('ä¸åŒç›´çº¿çš„è¯¯å·®å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("æœ€å°äºŒä¹˜æ³•æ‰¾åˆ°è¯¯å·®æœ€å°çš„ç›´çº¿!")

visualize_least_squares()
```

## å¤šå…ƒçº¿æ€§å›å½’ï¼šæ›´å¤æ‚çš„ç°å®

ç°å®ä¸­ï¼Œç›®æ ‡å¾€å¾€å—å¤šä¸ªå› ç´ å½±å“ï¼š

```python
def multiple_linear_regression_demo():
    """å¤šå…ƒçº¿æ€§å›å½’ç¤ºä¾‹"""
    
    # åˆ›å»ºå¤šç‰¹å¾æ•°æ®ï¼šæˆ¿ä»·å—é¢ç§¯ã€æˆ¿é—´æ•°ã€æ¥¼å±‚å½±å“
    np.random.seed(42)
    n_samples = 200
    
    area = np.random.uniform(50, 200, n_samples)        # é¢ç§¯
    rooms = np.random.randint(1, 6, n_samples)          # æˆ¿é—´æ•°
    floor = np.random.randint(1, 21, n_samples)         # æ¥¼å±‚
    
    # çœŸå®çš„ä»·æ ¼æ¨¡å‹ï¼ˆæˆ‘ä»¬é¢„å…ˆè®¾å®šçš„è§„å¾‹ï¼‰
    true_price = (2.0 * area +           # é¢ç§¯å½±å“ï¼š2ä¸‡/å¹³ç±³
                  5.0 * rooms +          # æˆ¿é—´å½±å“ï¼š5ä¸‡/é—´
                  0.2 * floor +          # æ¥¼å±‚å½±å“ï¼š0.2ä¸‡/å±‚
                  30)                    # åŸºç¡€ä»·æ ¼ï¼š30ä¸‡
    
    # æ·»åŠ å™ªå£°
    noise = np.random.normal(0, 8, n_samples)
    observed_price = true_price + noise
    
    # ç»„ç»‡æ•°æ®
    X = np.column_stack([area, rooms, floor])
    feature_names = ['é¢ç§¯', 'æˆ¿é—´æ•°', 'æ¥¼å±‚']
    
    # è®­ç»ƒæ¨¡å‹
    model = LinearRegression()
    model.fit(X, observed_price)
    
    # é¢„æµ‹
    predictions = model.predict(X)
    
    # åˆ†æç»“æœ
    print("å¤šå…ƒçº¿æ€§å›å½’ç»“æœåˆ†æ:")
    print("=" * 50)
    print("çœŸå®ç³»æ•° vs å­¦ä¹ åˆ°çš„ç³»æ•°:")
    true_coeffs = [2.0, 5.0, 0.2]
    for i, name in enumerate(feature_names):
        print(f"{name:>6}: çœŸå®={true_coeffs[i]:6.2f}, å­¦ä¹ ={model.coef_[i]:6.2f}")
    
    print(f"æˆªè·é¡¹: çœŸå®={30:6.2f}, å­¦ä¹ ={model.intercept_:6.2f}")
    print(f"RÂ²åˆ†æ•°: {model.score(X, observed_price):.3f}")
    
    # å¯è§†åŒ–å„ç‰¹å¾çš„å½±å“
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for i, (name, ax) in enumerate(zip(feature_names, axes)):
        ax.scatter(X[:, i], observed_price, alpha=0.6)
        
        # ç»˜åˆ¶å•å˜é‡è¶‹åŠ¿ï¼ˆå›ºå®šå…¶ä»–å˜é‡ï¼‰
        x_range = np.linspace(X[:, i].min(), X[:, i].max(), 100)
        
        # åˆ›å»ºé¢„æµ‹ç”¨çš„æ•°æ®ï¼ˆå…¶ä»–ç‰¹å¾å–å‡å€¼ï¼‰
        X_temp = np.zeros((100, 3))
        X_temp[:, i] = x_range
        for j in range(3):
            if j != i:
                X_temp[:, j] = np.mean(X[:, j])
        
        y_trend = model.predict(X_temp)
        ax.plot(x_range, y_trend, 'r-', linewidth=2)
        
        ax.set_xlabel(name)
        ax.set_ylabel('æˆ¿ä»· (ä¸‡å…ƒ)')
        ax.set_title(f'{name}ä¸æˆ¿ä»·çš„å…³ç³»')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    feature_importance = np.abs(model.coef_)
    plt.figure(figsize=(8, 5))
    bars = plt.bar(feature_names, feature_importance, color=['skyblue', 'lightgreen', 'lightcoral'])
    plt.title('ç‰¹å¾é‡è¦æ€§ï¼ˆç³»æ•°ç»å¯¹å€¼ï¼‰')
    plt.ylabel('ç³»æ•°ç»å¯¹å€¼')
    
    # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, coef in zip(bars, model.coef_):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{coef:.2f}', ha='center', va='bottom')
    
    plt.grid(True, alpha=0.3)
    plt.show()

multiple_linear_regression_demo()
```

## æ‰‹åŠ¨å®ç°çº¿æ€§å›å½’

è®©æˆ‘ä»¬ä»å¤´å®ç°çº¿æ€§å›å½’ç®—æ³•ï¼Œç†è§£å…¶å†…éƒ¨å·¥ä½œåŸç†ï¼š

```python
class SimpleLinearRegression:
    """æ‰‹åŠ¨å®ç°çš„ç®€å•çº¿æ€§å›å½’"""
    
    def __init__(self):
        self.slope = None
        self.intercept = None
        
    def fit(self, X, y):
        """è®­ç»ƒæ¨¡å‹ï¼ˆæœ€å°äºŒä¹˜æ³•ï¼‰"""
        # è®¡ç®—å‡å€¼
        x_mean = np.mean(X)
        y_mean = np.mean(y)
        
        # è®¡ç®—åˆ†å­å’Œåˆ†æ¯
        numerator = np.sum((X - x_mean) * (y - y_mean))
        denominator = np.sum((X - x_mean) ** 2)
        
        # è®¡ç®—æ–œç‡å’Œæˆªè·
        self.slope = numerator / denominator
        self.intercept = y_mean - self.slope * x_mean
        
        return self
    
    def predict(self, X):
        """é¢„æµ‹"""
        return self.slope * X + self.intercept
    
    def score(self, X, y):
        """è®¡ç®—RÂ²åˆ†æ•°"""
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)  # æ®‹å·®å¹³æ–¹å’Œ
        ss_tot = np.sum((y - np.mean(y)) ** 2)  # æ€»å¹³æ–¹å’Œ
        return 1 - (ss_res / ss_tot)

class MultipleLinearRegression:
    """æ‰‹åŠ¨å®ç°çš„å¤šå…ƒçº¿æ€§å›å½’"""
    
    def __init__(self):
        self.coefficients = None
        self.intercept = None
    
    def fit(self, X, y):
        """ä½¿ç”¨æ­£è§„æ–¹ç¨‹è®­ç»ƒæ¨¡å‹"""
        # æ·»åŠ åç½®åˆ—ï¼ˆå…¨1åˆ—ï¼‰
        X_with_bias = np.c_[np.ones(X.shape[0]), X]
        
        # æ­£è§„æ–¹ç¨‹: Î¸ = (X^T X)^(-1) X^T y
        XtX = X_with_bias.T @ X_with_bias
        Xty = X_with_bias.T @ y
        theta = np.linalg.solve(XtX, Xty)
        
        self.intercept = theta[0]
        self.coefficients = theta[1:]
        
        return self
    
    def predict(self, X):
        """é¢„æµ‹"""
        return X @ self.coefficients + self.intercept
    
    def score(self, X, y):
        """è®¡ç®—RÂ²åˆ†æ•°"""
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        return 1 - (ss_res / ss_tot)

# æµ‹è¯•æ‰‹åŠ¨å®ç°
def test_manual_implementation():
    """æµ‹è¯•æ‰‹åŠ¨å®ç°çš„çº¿æ€§å›å½’"""
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    X_simple = np.random.randn(100)
    y_simple = 2 * X_simple + 1 + 0.1 * np.random.randn(100)
    
    # ç®€å•çº¿æ€§å›å½’å¯¹æ¯”
    manual_simple = SimpleLinearRegression()
    manual_simple.fit(X_simple, y_simple)
    
    sklearn_simple = LinearRegression()
    sklearn_simple.fit(X_simple.reshape(-1, 1), y_simple)
    
    print("ç®€å•çº¿æ€§å›å½’å¯¹æ¯”:")
    print(f"æ‰‹åŠ¨å®ç° - æ–œç‡: {manual_simple.slope:.4f}, æˆªè·: {manual_simple.intercept:.4f}")
    print(f"sklearn  - æ–œç‡: {sklearn_simple.coef_[0]:.4f}, æˆªè·: {sklearn_simple.intercept:.4f}")
    print(f"RÂ²åˆ†æ•°å¯¹æ¯” - æ‰‹åŠ¨: {manual_simple.score(X_simple, y_simple):.4f}, sklearn: {sklearn_simple.score(X_simple.reshape(-1, 1), y_simple):.4f}")
    
    # å¤šå…ƒçº¿æ€§å›å½’å¯¹æ¯”
    X_multi = np.random.randn(100, 3)
    y_multi = X_multi @ [1, 2, -1] + 0.5 + 0.1 * np.random.randn(100)
    
    manual_multi = MultipleLinearRegression()
    manual_multi.fit(X_multi, y_multi)
    
    sklearn_multi = LinearRegression()
    sklearn_multi.fit(X_multi, y_multi)
    
    print(f"\nå¤šå…ƒçº¿æ€§å›å½’å¯¹æ¯”:")
    print(f"æ‰‹åŠ¨å®ç°ç³»æ•°: {manual_multi.coefficients}")
    print(f"sklearn ç³»æ•°: {sklearn_multi.coef_}")
    print(f"æˆªè·å¯¹æ¯” - æ‰‹åŠ¨: {manual_multi.intercept:.4f}, sklearn: {sklearn_multi.intercept:.4f}")

test_manual_implementation()
```

## æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£çº¿æ€§å›å½’

é™¤äº†æ­£è§„æ–¹ç¨‹ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£ï¼š

```python
class LinearRegressionGD:
    """ä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„çº¿æ€§å›å½’"""
    
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.coefficients = None
        self.intercept = None
        self.cost_history = []
    
    def fit(self, X, y):
        """ä½¿ç”¨æ¢¯åº¦ä¸‹é™è®­ç»ƒæ¨¡å‹"""
        n_samples, n_features = X.shape
        
        # åˆå§‹åŒ–å‚æ•°
        self.coefficients = np.zeros(n_features)
        self.intercept = 0
        
        # æ¢¯åº¦ä¸‹é™
        for i in range(self.max_iterations):
            # é¢„æµ‹
            y_pred = self.predict(X)
            
            # è®¡ç®—æˆæœ¬ï¼ˆå‡æ–¹è¯¯å·®ï¼‰
            cost = np.mean((y - y_pred) ** 2)
            self.cost_history.append(cost)
            
            # è®¡ç®—æ¢¯åº¦
            dw = -(2/n_samples) * X.T @ (y - y_pred)
            db = -(2/n_samples) * np.sum(y - y_pred)
            
            # æ›´æ–°å‚æ•°
            self.coefficients -= self.learning_rate * dw
            self.intercept -= self.learning_rate * db
        
        return self
    
    def predict(self, X):
        return X @ self.coefficients + self.intercept

def demonstrate_gradient_descent():
    """æ¼”ç¤ºæ¢¯åº¦ä¸‹é™è¿‡ç¨‹"""
    
    # ç”Ÿæˆæ•°æ®
    np.random.seed(42)
    X = np.random.randn(100, 2)
    y = X @ [3, -2] + 1 + 0.1 * np.random.randn(100)
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    X_scaled = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    # è®­ç»ƒæ¨¡å‹
    model_gd = LinearRegressionGD(learning_rate=0.1, max_iterations=1000)
    model_gd.fit(X_scaled, y)
    
    # å¯¹æ¯”sklearn
    model_sklearn = LinearRegression()
    model_sklearn.fit(X_scaled, y)
    
    print("æ¢¯åº¦ä¸‹é™ vs sklearnå¯¹æ¯”:")
    print(f"æ¢¯åº¦ä¸‹é™ç³»æ•°: {model_gd.coefficients}")
    print(f"sklearnç³»æ•°: {model_sklearn.coef_}")
    print(f"æ¢¯åº¦ä¸‹é™æˆªè·: {model_gd.intercept:.4f}")
    print(f"sklearnæˆªè·: {model_sklearn.intercept:.4f}")
    
    # å¯è§†åŒ–æˆæœ¬å‡½æ•°æ”¶æ•›è¿‡ç¨‹
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(model_gd.cost_history)
    plt.title('æˆæœ¬å‡½æ•°æ”¶æ•›è¿‡ç¨‹')
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('å‡æ–¹è¯¯å·®')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(model_gd.cost_history[:100])  # å‰100æ¬¡è¿­ä»£
    plt.title('å‰100æ¬¡è¿­ä»£çš„æ”¶æ•›')
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('å‡æ–¹è¯¯å·®')
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

demonstrate_gradient_descent()
```

## æ¨¡å‹è¯„ä¼°ä¸è¯Šæ–­

### 1. è¯„ä¼°æŒ‡æ ‡

```python
def regression_metrics():
    """å›å½’æ¨¡å‹çš„å„ç§è¯„ä¼°æŒ‡æ ‡"""
    
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    X = np.random.randn(100, 3)
    y_true = X @ [1, 2, -1] + 1 + 0.2 * np.random.randn(100)
    
    # è®­ç»ƒæ¨¡å‹
    model = LinearRegression()
    model.fit(X, y_true)
    y_pred = model.predict(X)
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # æ‰‹åŠ¨è®¡ç®—
    residuals = y_true - y_pred
    ss_res = np.sum(residuals ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    
    print("å›å½’æ¨¡å‹è¯„ä¼°æŒ‡æ ‡:")
    print("=" * 40)
    print(f"å‡æ–¹è¯¯å·® (MSE):     {mse:.4f}")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE):   {rmse:.4f}")
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE):  {mae:.4f}")
    print(f"å†³å®šç³»æ•° (RÂ²):       {r2:.4f}")
    print(f"è°ƒæ•´RÂ² (n=100, p=3): {1 - (1-r2)*(100-1)/(100-3-1):.4f}")
    
    # å¯è§†åŒ–è¯„ä¼°
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # é¢„æµ‹vså®é™…
    axes[0, 0].scatter(y_true, y_pred, alpha=0.6)
    axes[0, 0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')
    axes[0, 0].set_xlabel('å®é™…å€¼')
    axes[0, 0].set_ylabel('é¢„æµ‹å€¼')
    axes[0, 0].set_title('é¢„æµ‹ vs å®é™…')
    axes[0, 0].grid(True, alpha=0.3)
    
    # æ®‹å·®å›¾
    axes[0, 1].scatter(y_pred, residuals, alpha=0.6)
    axes[0, 1].axhline(y=0, color='r', linestyle='--')
    axes[0, 1].set_xlabel('é¢„æµ‹å€¼')
    axes[0, 1].set_ylabel('æ®‹å·®')
    axes[0, 1].set_title('æ®‹å·®å›¾')
    axes[0, 1].grid(True, alpha=0.3)
    
    # æ®‹å·®åˆ†å¸ƒ
    axes[1, 0].hist(residuals, bins=20, alpha=0.7, edgecolor='black')
    axes[1, 0].set_xlabel('æ®‹å·®')
    axes[1, 0].set_ylabel('é¢‘æ•°')
    axes[1, 0].set_title('æ®‹å·®åˆ†å¸ƒ')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Q-Qå›¾ï¼ˆæ­£æ€æ€§æ£€éªŒï¼‰
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=axes[1, 1])
    axes[1, 1].set_title('Q-Qå›¾ï¼ˆæ­£æ€æ€§æ£€éªŒï¼‰')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

regression_metrics()
```

### 2. è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–

```python
def regularization_demo():
    """æ¼”ç¤ºæ­£åˆ™åŒ–å¯¹è¿‡æ‹Ÿåˆçš„å½±å“"""
    
    from sklearn.linear_model import Ridge, Lasso
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import train_test_split
    
    # ç”Ÿæˆéçº¿æ€§æ•°æ®
    np.random.seed(42)
    X = np.linspace(0, 1, 100).reshape(-1, 1)
    y = np.sin(2 * np.pi * X).ravel() + 0.2 * np.random.randn(100)
    
    # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # åˆ›å»ºä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
    degrees = [1, 5, 10, 15]
    
    plt.figure(figsize=(16, 12))
    
    for i, degree in enumerate(degrees):
        # æ™®é€šçº¿æ€§å›å½’
        poly_reg = Pipeline([
            ('poly', PolynomialFeatures(degree=degree)),
            ('linear', LinearRegression())
        ])
        
        # Ridgeå›å½’
        ridge_reg = Pipeline([
            ('poly', PolynomialFeatures(degree=degree)),
            ('ridge', Ridge(alpha=0.1))
        ])
        
        # Lassoå›å½’
        lasso_reg = Pipeline([
            ('poly', PolynomialFeatures(degree=degree)),
            ('lasso', Lasso(alpha=0.01))
        ])
        
        # è®­ç»ƒæ¨¡å‹
        poly_reg.fit(X_train, y_train)
        ridge_reg.fit(X_train, y_train)
        lasso_reg.fit(X_train, y_train)
        
        # ç”Ÿæˆé¢„æµ‹æ›²çº¿
        X_plot = np.linspace(0, 1, 300).reshape(-1, 1)
        y_poly = poly_reg.predict(X_plot)
        y_ridge = ridge_reg.predict(X_plot)
        y_lasso = lasso_reg.predict(X_plot)
        
        # ç»˜å›¾
        plt.subplot(3, 4, i + 1)
        plt.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®')
        plt.scatter(X_test, y_test, alpha=0.6, color='red', label='æµ‹è¯•æ•°æ®')
        plt.plot(X_plot, y_poly, label='æ™®é€šå›å½’')
        plt.title(f'æ™®é€šå›å½’ (åº¦æ•°={degree})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, i + 5)
        plt.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®')
        plt.scatter(X_test, y_test, alpha=0.6, color='red', label='æµ‹è¯•æ•°æ®')
        plt.plot(X_plot, y_ridge, color='green', label='Ridgeå›å½’')
        plt.title(f'Ridgeå›å½’ (åº¦æ•°={degree})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, i + 9)
        plt.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®')
        plt.scatter(X_test, y_test, alpha=0.6, color='red', label='æµ‹è¯•æ•°æ®')
        plt.plot(X_plot, y_lasso, color='orange', label='Lassoå›å½’')
        plt.title(f'Lassoå›å½’ (åº¦æ•°={degree})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # è®¡ç®—åˆ†æ•°
        print(f"åº¦æ•° {degree}:")
        print(f"  æ™®é€šå›å½’ - è®­ç»ƒRÂ²: {poly_reg.score(X_train, y_train):.3f}, æµ‹è¯•RÂ²: {poly_reg.score(X_test, y_test):.3f}")
        print(f"  Ridgeå›å½’ - è®­ç»ƒRÂ²: {ridge_reg.score(X_train, y_train):.3f}, æµ‹è¯•RÂ²: {ridge_reg.score(X_test, y_test):.3f}")
        print(f"  Lassoå›å½’ - è®­ç»ƒRÂ²: {lasso_reg.score(X_train, y_train):.3f}, æµ‹è¯•RÂ²: {lasso_reg.score(X_test, y_test):.3f}")
        print()
    
    plt.tight_layout()
    plt.show()

regularization_demo()
```

## å®é™…åº”ç”¨æ¡ˆä¾‹

### æˆ¿ä»·é¢„æµ‹å®Œæ•´æ¡ˆä¾‹

```python
def house_price_prediction():
    """å®Œæ•´çš„æˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹"""
    
    # åˆ›å»ºæ›´çœŸå®çš„æˆ¿ä»·æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    
    # ç‰¹å¾å·¥ç¨‹
    area = np.random.normal(120, 30, n_samples)  # é¢ç§¯
    rooms = np.random.randint(1, 6, n_samples)   # æˆ¿é—´æ•°
    age = np.random.randint(0, 30, n_samples)    # æˆ¿é¾„
    distance = np.random.exponential(5, n_samples)  # è·ç¦»å¸‚ä¸­å¿ƒè·ç¦»
    
    # å¤æ‚çš„ä»·æ ¼æ¨¡å‹
    base_price = 50  # åŸºç¡€ä»·æ ¼
    area_effect = 2.5 * area  # é¢ç§¯æ•ˆåº”
    room_effect = 8 * rooms   # æˆ¿é—´æ•ˆåº”
    age_effect = -0.8 * age   # æˆ¿é¾„æ•ˆåº”ï¼ˆè´Ÿæ•ˆåº”ï¼‰
    distance_effect = -2 * distance  # è·ç¦»æ•ˆåº”ï¼ˆè´Ÿæ•ˆåº”ï¼‰
    
    # éçº¿æ€§æ•ˆåº”
    luxury_effect = np.where(area > 150, (area - 150) * 1.5, 0)  # è±ªå®…æ•ˆåº”
    
    true_price = (base_price + area_effect + room_effect + 
                  age_effect + distance_effect + luxury_effect)
    
    # æ·»åŠ å™ªå£°
    noise = np.random.normal(0, 10, n_samples)
    observed_price = true_price + noise
    
    # ç»„ç»‡æ•°æ®
    X = np.column_stack([area, rooms, age, distance])
    feature_names = ['é¢ç§¯(mÂ²)', 'æˆ¿é—´æ•°', 'æˆ¿é¾„(å¹´)', 'è·ç¦»(km)']
    
    # æ•°æ®åˆ†å‰²
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, observed_price, test_size=0.2, random_state=42)
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è®­ç»ƒå¤šä¸ªæ¨¡å‹
    models = {
        'çº¿æ€§å›å½’': LinearRegression(),
        'Ridgeå›å½’': Ridge(alpha=1.0),
        'Lassoå›å½’': Lasso(alpha=0.1)
    }
    
    results = {}
    
    print("æˆ¿ä»·é¢„æµ‹æ¨¡å‹å¯¹æ¯”:")
    print("=" * 60)
    
    for name, model in models.items():
        # è®­ç»ƒ
        model.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹
        y_train_pred = model.predict(X_train_scaled)
        y_test_pred = model.predict(X_test_scaled)
        
        # è¯„ä¼°
        train_r2 = model.score(X_train_scaled, y_train)
        test_r2 = model.score(X_test_scaled, y_test)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        
        results[name] = {
            'model': model,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'train_rmse': train_rmse,
            'test_rmse': test_rmse
        }
        
        print(f"{name}:")
        print(f"  è®­ç»ƒRÂ²: {train_r2:.3f}, æµ‹è¯•RÂ²: {test_r2:.3f}")
        print(f"  è®­ç»ƒRMSE: {train_rmse:.2f}, æµ‹è¯•RMSE: {test_rmse:.2f}")
        print(f"  ç‰¹å¾ç³»æ•°: {model.coef_}")
        print()
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), key=lambda x: results[x]['test_r2'])
    best_model = results[best_model_name]['model']
    
    print(f"æœ€ä½³æ¨¡å‹: {best_model_name}")
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    feature_importance = np.abs(best_model.coef_)
    
    plt.figure(figsize=(15, 10))
    
    # ç‰¹å¾é‡è¦æ€§
    plt.subplot(2, 3, 1)
    bars = plt.bar(feature_names, feature_importance)
    plt.title('ç‰¹å¾é‡è¦æ€§')
    plt.xticks(rotation=45)
    for bar, coef in zip(bars, best_model.coef_):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{coef:.2f}', ha='center', va='bottom')
    
    # é¢„æµ‹vså®é™…
    plt.subplot(2, 3, 2)
    y_test_pred = best_model.predict(X_test_scaled)
    plt.scatter(y_test, y_test_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.xlabel('å®é™…ä»·æ ¼')
    plt.ylabel('é¢„æµ‹ä»·æ ¼')
    plt.title('é¢„æµ‹æ•ˆæœ')
    
    # æ®‹å·®åˆ†æ
    plt.subplot(2, 3, 3)
    residuals = y_test - y_test_pred
    plt.scatter(y_test_pred, residuals, alpha=0.6)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('é¢„æµ‹ä»·æ ¼')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®åˆ†æ')
    
    # å„ç‰¹å¾ä¸ä»·æ ¼çš„å…³ç³»
    for i in range(4):
        plt.subplot(2, 3, 4 + i if i < 2 else 5 + i)
        plt.scatter(X_test[:, i], y_test, alpha=0.6)
        plt.xlabel(feature_names[i])
        plt.ylabel('ä»·æ ¼')
        plt.title(f'{feature_names[i]}ä¸ä»·æ ¼å…³ç³»')
    
    plt.tight_layout()
    plt.show()
    
    # å®é™…é¢„æµ‹ç¤ºä¾‹
    print("\nå®é™…é¢„æµ‹ç¤ºä¾‹:")
    print("-" * 30)
    
    # å‡ ä¸ªç¤ºä¾‹æˆ¿å±‹
    examples = [
        [100, 3, 5, 2],   # 100å¹³ç±³ï¼Œ3å®¤ï¼Œ5å¹´æˆ¿é¾„ï¼Œè·ç¦»å¸‚ä¸­å¿ƒ2km
        [150, 4, 10, 1],  # 150å¹³ç±³ï¼Œ4å®¤ï¼Œ10å¹´æˆ¿é¾„ï¼Œè·ç¦»å¸‚ä¸­å¿ƒ1km
        [80, 2, 20, 8],   # 80å¹³ç±³ï¼Œ2å®¤ï¼Œ20å¹´æˆ¿é¾„ï¼Œè·ç¦»å¸‚ä¸­å¿ƒ8km
    ]
    
    for i, example in enumerate(examples):
        example_scaled = scaler.transform([example])
        predicted_price = best_model.predict(example_scaled)[0]
        
        print(f"æˆ¿å±‹ {i+1}: {feature_names[0]}={example[0]}, {feature_names[1]}={example[1]}, "
              f"{feature_names[2]}={example[2]}, {feature_names[3]}={example[3]:.1f}")
        print(f"é¢„æµ‹ä»·æ ¼: {predicted_price:.1f}ä¸‡å…ƒ")
        print()

house_price_prediction()
```

## çº¿æ€§å›å½’çš„å‡è®¾ä¸å±€é™æ€§

### åŸºæœ¬å‡è®¾

1. **çº¿æ€§å…³ç³»**ï¼šç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»
2. **ç‹¬ç«‹æ€§**ï¼šè§‚æµ‹å€¼ä¹‹é—´ç›¸äº’ç‹¬ç«‹
3. **åŒæ–¹å·®æ€§**ï¼šæ®‹å·®çš„æ–¹å·®æ˜¯å¸¸æ•°
4. **æ­£æ€æ€§**ï¼šæ®‹å·®æœä»æ­£æ€åˆ†å¸ƒ
5. **æ— å¤šé‡å…±çº¿æ€§**ï¼šç‰¹å¾ä¹‹é—´ä¸é«˜åº¦ç›¸å…³

### å‡è®¾æ£€éªŒ

```python
def check_assumptions():
    """æ£€éªŒçº¿æ€§å›å½’çš„åŸºæœ¬å‡è®¾"""
    
    # ç”Ÿæˆè¿åå‡è®¾çš„æ•°æ®
    np.random.seed(42)
    n = 200
    x = np.linspace(0, 10, n)
    
    # è¿åçº¿æ€§å‡è®¾ï¼ˆçœŸå®å…³ç³»æ˜¯äºŒæ¬¡çš„ï¼‰
    y_nonlinear = 0.5 * x**2 + 2 * x + 1 + np.random.normal(0, 2, n)
    
    # è¿ååŒæ–¹å·®å‡è®¾ï¼ˆæ–¹å·®éšxå¢å¤§ï¼‰
    y_heteroskedastic = 2 * x + 1 + np.random.normal(0, 0.5 * x, n)
    
    # æ­£å¸¸æ•°æ®
    y_normal = 2 * x + 1 + np.random.normal(0, 2, n)
    
    datasets = [
        ('æ­£å¸¸æ•°æ®', y_normal),
        ('éçº¿æ€§æ•°æ®', y_nonlinear),
        ('å¼‚æ–¹å·®æ•°æ®', y_heteroskedastic)
    ]
    
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))
    
    for i, (name, y) in enumerate(datasets):
        # æ‹Ÿåˆæ¨¡å‹
        model = LinearRegression()
        X = x.reshape(-1, 1)
        model.fit(X, y)
        y_pred = model.predict(X)
        residuals = y - y_pred
        
        # åŸæ•°æ®æ•£ç‚¹å›¾
        axes[i, 0].scatter(x, y, alpha=0.6)
        axes[i, 0].plot(x, y_pred, 'r-', linewidth=2)
        axes[i, 0].set_title(f'{name} - æ‹Ÿåˆç»“æœ')
        axes[i, 0].set_xlabel('x')
        axes[i, 0].set_ylabel('y')
        axes[i, 0].grid(True, alpha=0.3)
        
        # æ®‹å·®å›¾
        axes[i, 1].scatter(y_pred, residuals, alpha=0.6)
        axes[i, 1].axhline(y=0, color='r', linestyle='--')
        axes[i, 1].set_title(f'{name} - æ®‹å·®å›¾')
        axes[i, 1].set_xlabel('é¢„æµ‹å€¼')
        axes[i, 1].set_ylabel('æ®‹å·®')
        axes[i, 1].grid(True, alpha=0.3)
        
        # Q-Qå›¾
        from scipy import stats
        stats.probplot(residuals, dist="norm", plot=axes[i, 2])
        axes[i, 2].set_title(f'{name} - Q-Qå›¾')
        axes[i, 2].grid(True, alpha=0.3)
        
        # ç»Ÿè®¡æ£€éªŒ
        _, p_value = stats.shapiro(residuals)
        print(f"{name}:")
        print(f"  Shapiro-Wilkæ­£æ€æ€§æ£€éªŒ på€¼: {p_value:.4f}")
        print(f"  RÂ²åˆ†æ•°: {model.score(X, y):.4f}")
        print()
    
    plt.tight_layout()
    plt.show()

check_assumptions()
```

## æ€»ç»“ï¼šçº¿æ€§å›å½’çš„æ ¸å¿ƒè¦ç‚¹

çº¿æ€§å›å½’å°±åƒæ˜¯ä¸€ä¸ª**æ•°æ®å…³ç³»çš„ç¿»è¯‘å®˜**ï¼š

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

1. **å¯»æ‰¾æœ€ä½³ç›´çº¿**ï¼šç”¨ä¸€æ¡ç›´çº¿æœ€å¥½åœ°æè¿°æ•°æ®å…³ç³»
2. **æœ€å°åŒ–è¯¯å·®**ï¼šè®©é¢„æµ‹å€¼å°½å¯èƒ½æ¥è¿‘çœŸå®å€¼
3. **é‡åŒ–å½±å“**ï¼šæ¯ä¸ªç‰¹å¾å¯¹ç›®æ ‡çš„å½±å“ç¨‹åº¦

### ğŸ“Š å…³é”®æ¦‚å¿µ

- **æ–œç‡/ç³»æ•°**ï¼šç‰¹å¾çš„é‡è¦æ€§å’Œå½±å“æ–¹å‘
- **æˆªè·**ï¼šåŸºç¡€æ°´å¹³/èµ·å§‹å€¼
- **RÂ²**ï¼šæ¨¡å‹è§£é‡Šæ•°æ®å˜å¼‚çš„æ¯”ä¾‹
- **æ®‹å·®**ï¼šé¢„æµ‹ä¸å®é™…çš„å·®å¼‚

### ğŸ’¡ å®ç”¨æŠ€å·§

1. **æ•°æ®é¢„å¤„ç†**ï¼šæ ‡å‡†åŒ–ã€å¤„ç†ç¼ºå¤±å€¼
2. **ç‰¹å¾å·¥ç¨‹**ï¼šåˆ›é€ æœ‰æ„ä¹‰çš„ç‰¹å¾
3. **æ¨¡å‹è¯Šæ–­**ï¼šæ£€æŸ¥å‡è®¾ã€åˆ†ææ®‹å·®
4. **æ­£åˆ™åŒ–**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
5. **äº¤å‰éªŒè¯**ï¼šè¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### ğŸª åº”ç”¨åœºæ™¯

- **é¢„æµ‹é—®é¢˜**ï¼šæˆ¿ä»·ã€é”€é‡ã€æˆç»©ç­‰
- **ç‰¹å¾é‡è¦æ€§åˆ†æ**ï¼šäº†è§£å“ªäº›å› ç´ æœ€é‡è¦
- **è¶‹åŠ¿åˆ†æ**ï¼šç†è§£å˜é‡é—´çš„å…³ç³»
- **åŸºå‡†æ¨¡å‹**ï¼šä½œä¸ºå¤æ‚æ¨¡å‹çš„å¯¹æ¯”åŸºçº¿

### ğŸ”§ è®°å¿†å£è¯€

**"æ‰¾ç›´çº¿ï¼Œç®—è¯¯å·®ï¼Œè°ƒå‚æ•°ï¼ŒéªŒæ•ˆæœ"**

çº¿æ€§å›å½’è™½ç„¶ç®€å•ï¼Œå´æ˜¯æœºå™¨å­¦ä¹ çš„åŸºçŸ³ã€‚æŒæ¡äº†çº¿æ€§å›å½’ï¼Œä½ å°±æ‹¿åˆ°äº†è¿›å…¥æœºå™¨å­¦ä¹ ä¸–ç•Œçš„ç¬¬ä¸€æŠŠé’¥åŒ™ï¼

---

**ä½œè€…**: meimeitou  
