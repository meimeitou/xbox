+++
title = 'ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰'
weight = 2
math = true
+++

ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼šæ•°æ®é™ç»´çš„è‰ºæœ¯å¤§å¸ˆ

- [å¼•è¨€](#å¼•è¨€)
- [ä»€ä¹ˆæ˜¯ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼Ÿ](#ä»€ä¹ˆæ˜¯ä¸»æˆåˆ†åˆ†æpca)
  - [ç”Ÿæ´»ä¸­çš„ç±»æ¯”](#ç”Ÿæ´»ä¸­çš„ç±»æ¯”)
- [æ ¸å¿ƒæ€æƒ³ï¼šå¯»æ‰¾æ•°æ®çš„"ä¸»è¦æ–¹å‘"](#æ ¸å¿ƒæ€æƒ³å¯»æ‰¾æ•°æ®çš„ä¸»è¦æ–¹å‘)
  - [ä»€ä¹ˆæ˜¯ä¸»æˆåˆ†ï¼Ÿ](#ä»€ä¹ˆæ˜¯ä¸»æˆåˆ†)
  - [2Dç¤ºä¾‹ï¼šç†è§£ä¸»æˆåˆ†](#2dç¤ºä¾‹ç†è§£ä¸»æˆåˆ†)
- [PCAçš„æ•°å­¦åŸç†ï¼ˆç®€åŒ–ç‰ˆï¼‰](#pcaçš„æ•°å­¦åŸç†ç®€åŒ–ç‰ˆ)
  - [æ ¸å¿ƒç›®æ ‡](#æ ¸å¿ƒç›®æ ‡)
  - [æ­¥éª¤åˆ†è§£](#æ­¥éª¤åˆ†è§£)
- [è¯¦ç»†ä»£ç å®ç°](#è¯¦ç»†ä»£ç å®ç°)
  - [å®Œæ•´çš„PCAç±»](#å®Œæ•´çš„pcaç±»)
- [å®é™…åº”ç”¨ç¤ºä¾‹](#å®é™…åº”ç”¨ç¤ºä¾‹)
  - [1. å›¾åƒå‹ç¼©](#1-å›¾åƒå‹ç¼©)
  - [2. æ•°æ®å¯è§†åŒ–](#2-æ•°æ®å¯è§†åŒ–)
- [å¦‚ä½•é€‰æ‹©ä¸»æˆåˆ†æ•°é‡ï¼Ÿ](#å¦‚ä½•é€‰æ‹©ä¸»æˆåˆ†æ•°é‡)
  - [1. è§£é‡Šæ–¹å·®æ¯”ä¾‹æ³•](#1-è§£é‡Šæ–¹å·®æ¯”ä¾‹æ³•)
  - [2. è‚˜éƒ¨æ³•åˆ™](#2-è‚˜éƒ¨æ³•åˆ™)
- [PCAçš„ä¼˜ç¼ºç‚¹åˆ†æ](#pcaçš„ä¼˜ç¼ºç‚¹åˆ†æ)
  - [ä¼˜ç‚¹ âœ…](#ä¼˜ç‚¹-)
  - [ç¼ºç‚¹ âŒ](#ç¼ºç‚¹-)
  - [ä½•æ—¶ä½¿ç”¨PCAï¼Ÿ](#ä½•æ—¶ä½¿ç”¨pca)
- [å®æˆ˜æŠ€å·§å’Œæœ€ä½³å®è·µ](#å®æˆ˜æŠ€å·§å’Œæœ€ä½³å®è·µ)
  - [1. æ•°æ®é¢„å¤„ç†](#1-æ•°æ®é¢„å¤„ç†)
  - [2. å¤„ç†ç¼ºå¤±å€¼](#2-å¤„ç†ç¼ºå¤±å€¼)
- [æ€»ç»“ï¼šPCAçš„æ ¸å¿ƒæ€æƒ³](#æ€»ç»“pcaçš„æ ¸å¿ƒæ€æƒ³)
  - [è®°å¿†å£è¯€](#è®°å¿†å£è¯€)

## å¼•è¨€

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æœ‰ä¸€ä¸ªè£…æ»¡å„ç§ç‰©å“çš„è¡Œæç®±ï¼Œä½†èˆªç©ºå…¬å¸çªç„¶è¯´åªèƒ½å¸¦ä¸€åŠçš„é‡é‡ã€‚ä½ ä¼šæ€ä¹ˆåŠï¼Ÿèªæ˜çš„åšæ³•æ˜¯ï¼š**ä¿ç•™æœ€é‡è¦çš„ç‰©å“ï¼Œä¸¢å¼ƒä¸é‡è¦çš„ï¼ŒåŒæ—¶å°½å¯èƒ½ä¿æŒè¡Œæç®±çš„"å®Œæ•´åŠŸèƒ½"**ã€‚

è¿™å°±æ˜¯ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è¦è§£å†³çš„é—®é¢˜ï¼å®ƒæ˜¯æ•°æ®ç§‘å­¦ä¸­æœ€é‡è¦çš„é™ç»´æŠ€æœ¯ä¹‹ä¸€ã€‚

## ä»€ä¹ˆæ˜¯ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼Ÿ

ä¸»æˆåˆ†åˆ†æï¼ˆPrincipal Component Analysisï¼ŒPCAï¼‰æ˜¯ä¸€ç§**æ•°æ®é™ç»´**æŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿï¼š

- å°†é«˜ç»´æ•°æ®è½¬æ¢ä¸ºä½ç»´æ•°æ®
- **ä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯**
- **å»é™¤å†—ä½™å’Œå™ªå£°**
- å‘ç°æ•°æ®ä¸­çš„**ä¸»è¦å˜åŒ–æ–¹å‘**

### ç”Ÿæ´»ä¸­çš„ç±»æ¯”

1. **æ‘„å½±å¸ˆçš„è§†è§’é€‰æ‹©** ğŸ“¸
   - é€‰æ‹©æœ€èƒ½è¡¨ç°ä¸»é¢˜çš„è§’åº¦
   - ä¸€å¼ å¥½ç…§ç‰‡èƒ½å±•ç°äº‹ç‰©çš„ä¸»è¦ç‰¹å¾

2. **åœ°å›¾çš„åˆ¶ä½œ** ğŸ—ºï¸
   - 3Dçš„åœ°çƒè¡¨é¢ â†’ 2Dçš„å¹³é¢åœ°å›¾
   - ä¿ç•™æœ€é‡è¦çš„åœ°ç†ä¿¡æ¯
   - ä¸åŒæŠ•å½±æ–¹å¼é€‚åˆä¸åŒç”¨é€”

3. **ç®€å†çš„æ’°å†™** ğŸ“
   - ä¸°å¯Œçš„äººç”Ÿç»å† â†’ ä¸€é¡µçº¸çš„ç®€å†
   - çªå‡ºæœ€é‡è¦çš„æŠ€èƒ½å’Œç»éªŒ
   - ç”¨æœ€å°‘çš„ä¿¡æ¯å±•ç°æœ€å¤§çš„ä»·å€¼

## æ ¸å¿ƒæ€æƒ³ï¼šå¯»æ‰¾æ•°æ®çš„"ä¸»è¦æ–¹å‘"

### ä»€ä¹ˆæ˜¯ä¸»æˆåˆ†ï¼Ÿ

**ä¸»æˆåˆ†**å°±æ˜¯æ•°æ®å˜åŒ–æœ€å¤§çš„æ–¹å‘ï¼š

1. **ç¬¬ä¸€ä¸»æˆåˆ†**ï¼šæ•°æ®å˜åŒ–æœ€å¤§çš„æ–¹å‘
2. **ç¬¬äºŒä¸»æˆåˆ†**ï¼šä¸ç¬¬ä¸€ä¸»æˆåˆ†å‚ç›´ï¼Œå˜åŒ–ç¬¬äºŒå¤§çš„æ–¹å‘
3. **ç¬¬ä¸‰ä¸»æˆåˆ†**ï¼šä¸å‰ä¸¤ä¸ªéƒ½å‚ç›´ï¼Œå˜åŒ–ç¬¬ä¸‰å¤§çš„æ–¹å‘
4. ä»¥æ­¤ç±»æ¨...

### 2Dç¤ºä¾‹ï¼šç†è§£ä¸»æˆåˆ†

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import make_blobs

# åˆ›å»ºç¤ºä¾‹æ•°æ®
np.random.seed(42)
X, _ = make_blobs(n_samples=100, centers=1, n_features=2, 
                  cluster_std=2.0, random_state=42)

# è®©æ•°æ®æœ‰æ˜æ˜¾çš„æ–¹å‘æ€§
rotation_matrix = np.array([[1, 0.5], [0, 1]])
X = X @ rotation_matrix

# åº”ç”¨PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# åŸå§‹æ•°æ®
axes[0].scatter(X[:, 0], X[:, 1], alpha=0.7)
axes[0].set_title('åŸå§‹æ•°æ®')
axes[0].set_xlabel('ç‰¹å¾1')
axes[0].set_ylabel('ç‰¹å¾2')
axes[0].grid(True)

# åŸå§‹æ•°æ® + ä¸»æˆåˆ†æ–¹å‘
axes[1].scatter(X[:, 0], X[:, 1], alpha=0.7)

# ç»˜åˆ¶ä¸»æˆåˆ†æ–¹å‘
mean_point = np.mean(X, axis=0)
for i, (component, variance) in enumerate(zip(pca.components_, pca.explained_variance_)):
    axes[1].arrow(mean_point[0], mean_point[1], 
                  component[0] * variance, component[1] * variance,
                  head_width=0.3, head_length=0.3, 
                  fc=f'C{i+1}', ec=f'C{i+1}', 
                  label=f'ä¸»æˆåˆ†{i+1}')

axes[1].set_title('åŸå§‹æ•°æ® + ä¸»æˆåˆ†æ–¹å‘')
axes[1].set_xlabel('ç‰¹å¾1')
axes[1].set_ylabel('ç‰¹å¾2')
axes[1].legend()
axes[1].grid(True)

# PCAå˜æ¢åçš„æ•°æ®
axes[2].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)
axes[2].set_title('PCAå˜æ¢åçš„æ•°æ®')
axes[2].set_xlabel('ç¬¬ä¸€ä¸»æˆåˆ†')
axes[2].set_ylabel('ç¬¬äºŒä¸»æˆåˆ†')
axes[2].grid(True)

plt.tight_layout()
plt.show()

print("ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹:", pca.explained_variance_ratio_)
print("ç´¯è®¡è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹:", np.cumsum(pca.explained_variance_ratio_))
```

## PCAçš„æ•°å­¦åŸç†ï¼ˆç®€åŒ–ç‰ˆï¼‰

### æ ¸å¿ƒç›®æ ‡

PCAè¦æ‰¾åˆ°ä¸€ç»„æ–°çš„åæ ‡è½´ï¼ˆä¸»æˆåˆ†ï¼‰ï¼Œä½¿å¾—ï¼š

1. **æ•°æ®åœ¨æ–°è½´ä¸Šçš„æ–¹å·®æœ€å¤§**
2. **å„ä¸ªæ–°è½´äº’ç›¸å‚ç›´ï¼ˆæ­£äº¤ï¼‰**
3. **æŒ‰æ–¹å·®å¤§å°æ’åº**

### æ­¥éª¤åˆ†è§£

1. **æ•°æ®ä¸­å¿ƒåŒ–**ï¼šå°†æ•°æ®ç§»åŠ¨åˆ°åŸç‚¹
2. **è®¡ç®—åæ–¹å·®çŸ©é˜µ**ï¼šè¡¡é‡ç‰¹å¾ä¹‹é—´çš„å…³ç³»
3. **ç‰¹å¾å€¼åˆ†è§£**ï¼šæ‰¾åˆ°ä¸»è¦çš„å˜åŒ–æ–¹å‘
4. **é€‰æ‹©ä¸»æˆåˆ†**ï¼šä¿ç•™æœ€é‡è¦çš„å‡ ä¸ªæ–¹å‘
5. **æ•°æ®å˜æ¢**ï¼šå°†åŸå§‹æ•°æ®æŠ•å½±åˆ°æ–°çš„åæ ‡ç³»

```python
def manual_pca(X, n_components):
    """æ‰‹åŠ¨å®ç°PCAç®—æ³•"""
    
    # 1. æ•°æ®ä¸­å¿ƒåŒ–
    X_centered = X - np.mean(X, axis=0)
    print("æ­¥éª¤1: æ•°æ®ä¸­å¿ƒåŒ–å®Œæˆ")
    
    # 2. è®¡ç®—åæ–¹å·®çŸ©é˜µ
    cov_matrix = np.cov(X_centered.T)
    print("æ­¥éª¤2: åæ–¹å·®çŸ©é˜µè®¡ç®—å®Œæˆ")
    print("åæ–¹å·®çŸ©é˜µ:")
    print(cov_matrix)
    
    # 3. ç‰¹å¾å€¼åˆ†è§£
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
    print("æ­¥éª¤3: ç‰¹å¾å€¼åˆ†è§£å®Œæˆ")
    
    # 4. æŒ‰ç‰¹å¾å€¼å¤§å°æ’åºï¼ˆé™åºï¼‰
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    print("ç‰¹å¾å€¼ï¼ˆæŒ‰å¤§å°æ’åºï¼‰:", eigenvalues)
    print("è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹:", eigenvalues / np.sum(eigenvalues))
    
    # 5. é€‰æ‹©å‰n_componentsä¸ªä¸»æˆåˆ†
    selected_eigenvectors = eigenvectors[:, :n_components]
    
    # 6. æ•°æ®å˜æ¢
    X_pca = X_centered @ selected_eigenvectors
    
    return X_pca, selected_eigenvectors, eigenvalues

# ä½¿ç”¨æ‰‹åŠ¨å®ç°çš„PCA
X_manual, components, eigenvals = manual_pca(X, n_components=2)

print(f"\næ‰‹åŠ¨PCAç»“æœä¸sklearn PCAç»“æœçš„å·®å¼‚:")
print(f"æœ€å¤§å·®å¼‚: {np.max(np.abs(np.abs(X_manual) - np.abs(X_pca))):.10f}")
```

## è¯¦ç»†ä»£ç å®ç°

### å®Œæ•´çš„PCAç±»

```python
class SimplePCA:
    """ç®€åŒ–ç‰ˆPCAå®ç°"""
    
    def __init__(self, n_components):
        self.n_components = n_components
        self.components_ = None
        self.explained_variance_ = None
        self.explained_variance_ratio_ = None
        self.mean_ = None
    
    def fit(self, X):
        """è®­ç»ƒPCAæ¨¡å‹"""
        # ä¿å­˜å‡å€¼ç”¨äºä¸­å¿ƒåŒ–
        self.mean_ = np.mean(X, axis=0)
        
        # æ•°æ®ä¸­å¿ƒåŒ–
        X_centered = X - self.mean_
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        cov_matrix = np.cov(X_centered.T)
        
        # ç‰¹å¾å€¼åˆ†è§£
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        
        # æŒ‰ç‰¹å¾å€¼å¤§å°æ’åº
        idx = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        
        # ä¿å­˜ç»“æœ
        self.components_ = eigenvectors[:, :self.n_components].T
        self.explained_variance_ = eigenvalues[:self.n_components]
        self.explained_variance_ratio_ = self.explained_variance_ / np.sum(eigenvalues)
        
        return self
    
    def transform(self, X):
        """å°†æ•°æ®å˜æ¢åˆ°ä¸»æˆåˆ†ç©ºé—´"""
        X_centered = X - self.mean_
        return X_centered @ self.components_.T
    
    def fit_transform(self, X):
        """è®­ç»ƒå¹¶å˜æ¢æ•°æ®"""
        return self.fit(X).transform(X)
    
    def inverse_transform(self, X_pca):
        """ä»ä¸»æˆåˆ†ç©ºé—´å˜æ¢å›åŸå§‹ç©ºé—´"""
        return X_pca @ self.components_ + self.mean_

# æµ‹è¯•è‡ªåˆ¶PCA
simple_pca = SimplePCA(n_components=2)
X_simple = simple_pca.fit_transform(X)

print("è‡ªåˆ¶PCAçš„è§£é‡Šæ–¹å·®æ¯”ä¾‹:", simple_pca.explained_variance_ratio_)
```

## å®é™…åº”ç”¨ç¤ºä¾‹

### 1. å›¾åƒå‹ç¼©

```python
from sklearn.datasets import fetch_olivetti_faces

def pca_image_compression():
    """ä½¿ç”¨PCAè¿›è¡Œå›¾åƒå‹ç¼©"""
    
    # åŠ è½½äººè„¸æ•°æ®é›†
    faces = fetch_olivetti_faces()
    X_faces = faces.data  # æ¯è¡Œæ˜¯ä¸€ä¸ª64x64=4096ç»´çš„äººè„¸å›¾åƒ
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X_faces.shape}")
    print(f"æ¯ä¸ªå›¾åƒçš„ç»´åº¦: {X_faces.shape[1]}")
    
    # åº”ç”¨ä¸åŒå‹ç¼©æ¯”çš„PCA
    compression_ratios = [50, 100, 200, 400]
    
    fig, axes = plt.subplots(2, len(compression_ratios) + 1, figsize=(15, 6))
    
    # æ˜¾ç¤ºåŸå§‹å›¾åƒ
    original_image = X_faces[0].reshape(64, 64)
    axes[0, 0].imshow(original_image, cmap='gray')
    axes[0, 0].set_title('åŸå§‹å›¾åƒ\n(4096ç»´)')
    axes[0, 0].axis('off')
    
    # æ˜¾ç¤ºä¸åŒå‹ç¼©æ¯”çš„ç»“æœ
    for i, n_components in enumerate(compression_ratios):
        # PCAå‹ç¼©
        pca = PCA(n_components=n_components)
        X_compressed = pca.fit_transform(X_faces)
        X_reconstructed = pca.inverse_transform(X_compressed)
        
        # é‡æ„å›¾åƒ
        reconstructed_image = X_reconstructed[0].reshape(64, 64)
        
        # è®¡ç®—å‹ç¼©æ¯”å’Œè¯¯å·®
        compression_ratio = X_faces.shape[1] / n_components
        mse = np.mean((X_faces[0] - X_reconstructed[0])**2)
        explained_variance = np.sum(pca.explained_variance_ratio_)
        
        # æ˜¾ç¤ºç»“æœ
        axes[0, i+1].imshow(reconstructed_image, cmap='gray')
        axes[0, i+1].set_title(f'{n_components}ç»´\nå‹ç¼©æ¯”: {compression_ratio:.1f}:1')
        axes[0, i+1].axis('off')
        
        axes[1, i+1].bar(['ä¿ç•™ä¿¡æ¯', 'ä¸¢å¤±ä¿¡æ¯'], 
                        [explained_variance, 1-explained_variance])
        axes[1, i+1].set_title(f'ä¿¡æ¯ä¿ç•™: {explained_variance:.1%}')
        axes[1, i+1].set_ylim(0, 1)
    
    axes[1, 0].axis('off')
    plt.tight_layout()
    plt.show()

# pca_image_compression()  # å–æ¶ˆæ³¨é‡Šè¿è¡Œ
```

### 2. æ•°æ®å¯è§†åŒ–

```python
def pca_data_visualization():
    """ä½¿ç”¨PCAè¿›è¡Œé«˜ç»´æ•°æ®å¯è§†åŒ–"""
    
    # åˆ›å»ºé«˜ç»´æ•°æ®
    from sklearn.datasets import make_classification
    
    X_high, y = make_classification(n_samples=300, n_features=20, 
                                   n_informative=10, n_redundant=10,
                                   n_clusters_per_class=1, random_state=42)
    
    print(f"åŸå§‹æ•°æ®ç»´åº¦: {X_high.shape}")
    
    # é™ç»´åˆ°2Dè¿›è¡Œå¯è§†åŒ–
    pca_2d = PCA(n_components=2)
    X_2d = pca_2d.fit_transform(X_high)
    
    # é™ç»´åˆ°3D
    pca_3d = PCA(n_components=3)
    X_3d = pca_3d.fit_transform(X_high)
    
    # å¯è§†åŒ–
    fig = plt.figure(figsize=(15, 5))
    
    # 2Då¯è§†åŒ–
    ax1 = fig.add_subplot(131)
    scatter = ax1.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='viridis')
    ax1.set_title('PCA 2Då¯è§†åŒ–')
    ax1.set_xlabel('ç¬¬ä¸€ä¸»æˆåˆ†')
    ax1.set_ylabel('ç¬¬äºŒä¸»æˆåˆ†')
    plt.colorbar(scatter, ax=ax1)
    
    # 3Då¯è§†åŒ–
    ax2 = fig.add_subplot(132, projection='3d')
    scatter3d = ax2.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=y, cmap='viridis')
    ax2.set_title('PCA 3Då¯è§†åŒ–')
    ax2.set_xlabel('ç¬¬ä¸€ä¸»æˆåˆ†')
    ax2.set_ylabel('ç¬¬äºŒä¸»æˆåˆ†')
    ax2.set_zlabel('ç¬¬ä¸‰ä¸»æˆåˆ†')
    
    # è§£é‡Šæ–¹å·®æ¯”ä¾‹
    ax3 = fig.add_subplot(133)
    n_components = min(10, X_high.shape[1])
    pca_full = PCA(n_components=n_components)
    pca_full.fit(X_high)
    
    ax3.bar(range(1, n_components+1), pca_full.explained_variance_ratio_)
    ax3.set_title('å„ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹')
    ax3.set_xlabel('ä¸»æˆåˆ†ç¼–å·')
    ax3.set_ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')
    
    plt.tight_layout()
    plt.show()
    
    print(f"å‰2ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®: {np.sum(pca_2d.explained_variance_ratio_):.2%}")
    print(f"å‰3ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®: {np.sum(pca_3d.explained_variance_ratio_):.2%}")

pca_data_visualization()
```

## å¦‚ä½•é€‰æ‹©ä¸»æˆåˆ†æ•°é‡ï¼Ÿ

### 1. è§£é‡Šæ–¹å·®æ¯”ä¾‹æ³•

```python
def choose_n_components_variance():
    """åŸºäºè§£é‡Šæ–¹å·®æ¯”ä¾‹é€‰æ‹©ä¸»æˆåˆ†æ•°é‡"""
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    from sklearn.datasets import load_digits
    X, y = load_digits(return_X_y=True)
    
    # è®¡ç®—æ‰€æœ‰ä¸»æˆåˆ†
    pca_full = PCA()
    pca_full.fit(X)
    
    # è®¡ç®—ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”ä¾‹
    cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)
    
    # ç»˜åˆ¶è§£é‡Šæ–¹å·®å›¾
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(cumsum_variance)+1), cumsum_variance, 'bo-')
    plt.axhline(y=0.95, color='r', linestyle='--', label='95%é˜ˆå€¼')
    plt.axhline(y=0.90, color='g', linestyle='--', label='90%é˜ˆå€¼')
    plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
    plt.ylabel('ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”ä¾‹')
    plt.title('ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”ä¾‹')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(range(1, 21), pca_full.explained_variance_ratio_[:20], 'ro-')
    plt.xlabel('ä¸»æˆåˆ†ç¼–å·')
    plt.ylabel('å•ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®æ¯”ä¾‹')
    plt.title('å‰20ä¸ªä¸»æˆåˆ†çš„è´¡çŒ®')
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰¾åˆ°è¾¾åˆ°95%è§£é‡Šæ–¹å·®çš„ä¸»æˆåˆ†æ•°é‡
    n_95 = np.argmax(cumsum_variance >= 0.95) + 1
    n_90 = np.argmax(cumsum_variance >= 0.90) + 1
    
    print(f"åŸå§‹ç»´åº¦: {X.shape[1]}")
    print(f"è¾¾åˆ°90%è§£é‡Šæ–¹å·®éœ€è¦: {n_90}ä¸ªä¸»æˆåˆ†")
    print(f"è¾¾åˆ°95%è§£é‡Šæ–¹å·®éœ€è¦: {n_95}ä¸ªä¸»æˆåˆ†")
    print(f"å‹ç¼©æ¯”(95%): {X.shape[1]/n_95:.1f}:1")

choose_n_components_variance()
```

### 2. è‚˜éƒ¨æ³•åˆ™

```python
def elbow_method_pca():
    """ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™é€‰æ‹©ä¸»æˆåˆ†æ•°é‡"""
    
    from sklearn.datasets import load_breast_cancer
    X, y = load_breast_cancer(return_X_y=True)
    
    # æ ‡å‡†åŒ–æ•°æ®
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # è®¡ç®—ä¸åŒä¸»æˆåˆ†æ•°é‡ä¸‹çš„é‡æ„è¯¯å·®
    n_components_range = range(1, min(21, X.shape[1]))
    reconstruction_errors = []
    
    for n in n_components_range:
        pca = PCA(n_components=n)
        X_pca = pca.fit_transform(X_scaled)
        X_reconstructed = pca.inverse_transform(X_pca)
        
        # è®¡ç®—é‡æ„è¯¯å·®
        mse = np.mean((X_scaled - X_reconstructed)**2)
        reconstruction_errors.append(mse)
    
    # ç»˜åˆ¶è‚˜éƒ¨å›¾
    plt.figure(figsize=(10, 6))
    plt.plot(n_components_range, reconstruction_errors, 'bo-')
    plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
    plt.ylabel('é‡æ„è¯¯å·®(MSE)')
    plt.title('PCAè‚˜éƒ¨æ³•åˆ™')
    plt.grid(True)
    
    # å¯»æ‰¾è‚˜éƒ¨ç‚¹
    differences = np.diff(reconstruction_errors)
    second_differences = np.diff(differences)
    elbow_point = np.argmax(second_differences) + 2
    
    plt.axvline(x=elbow_point, color='r', linestyle='--', 
                label=f'è‚˜éƒ¨ç‚¹: {elbow_point}ä¸ªä¸»æˆåˆ†')
    plt.legend()
    plt.show()
    
    print(f"å»ºè®®çš„ä¸»æˆåˆ†æ•°é‡: {elbow_point}")

elbow_method_pca()
```

## PCAçš„ä¼˜ç¼ºç‚¹åˆ†æ

### ä¼˜ç‚¹ âœ…

1. **é™ç»´æ•ˆæœå¥½**ï¼šèƒ½æ˜¾è‘—å‡å°‘æ•°æ®ç»´åº¦
2. **å»é™¤å†—ä½™**ï¼šè‡ªåŠ¨å»é™¤ç‰¹å¾é—´çš„ç›¸å…³æ€§
3. **æ•°å­¦åŸºç¡€ç‰¢å›º**ï¼šåŸºäºçº¿æ€§ä»£æ•°ç†è®º
4. **è®¡ç®—æ•ˆç‡é«˜**ï¼šç®—æ³•å¤æ‚åº¦ç›¸å¯¹è¾ƒä½
5. **å¯è§£é‡Šæ€§å¼º**ï¼šä¸»æˆåˆ†æœ‰æ˜ç¡®çš„æ•°å­¦å«ä¹‰

### ç¼ºç‚¹ âŒ

1. **çº¿æ€§å‡è®¾**ï¼šåªèƒ½æ•æ‰çº¿æ€§å…³ç³»
2. **å…¨å±€æ–¹æ³•**ï¼šéœ€è¦æ‰€æœ‰æ•°æ®æ‰èƒ½è®¡ç®—
3. **å¯è§£é‡Šæ€§æœ‰é™**ï¼šä¸»æˆåˆ†é€šå¸¸æ˜¯åŸç‰¹å¾çš„å¤æ‚ç»„åˆ
4. **å¯¹ç¼©æ”¾æ•æ„Ÿ**ï¼šéœ€è¦é¢„å…ˆæ ‡å‡†åŒ–æ•°æ®
5. **ä¿¡æ¯ä¸¢å¤±**ï¼šé™ç»´å¿…ç„¶ä¸¢å¤±ä¸€äº›ä¿¡æ¯

### ä½•æ—¶ä½¿ç”¨PCAï¼Ÿ

```python
def when_to_use_pca():
    """æ¼”ç¤ºä½•æ—¶é€‚åˆä½¿ç”¨PCA"""
    
    # æ¡ˆä¾‹1: é«˜åº¦ç›¸å…³çš„ç‰¹å¾
    print("æ¡ˆä¾‹1: é«˜åº¦ç›¸å…³çš„ç‰¹å¾")
    n_samples = 1000
    x1 = np.random.randn(n_samples)
    x2 = x1 + 0.1 * np.random.randn(n_samples)  # ä¸x1é«˜åº¦ç›¸å…³
    x3 = 2 * x1 + 0.1 * np.random.randn(n_samples)  # ä¸x1çº¿æ€§ç›¸å…³
    
    X_correlated = np.column_stack([x1, x2, x3])
    
    pca_corr = PCA()
    pca_corr.fit(X_correlated)
    
    print("ç›¸å…³æ€§ç‰¹å¾çš„è§£é‡Šæ–¹å·®æ¯”ä¾‹:", pca_corr.explained_variance_ratio_)
    print("ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šäº†", f"{pca_corr.explained_variance_ratio_[0]:.1%}", "çš„æ–¹å·®\n")
    
    # æ¡ˆä¾‹2: å™ªå£°æ•°æ®
    print("æ¡ˆä¾‹2: å«å™ªå£°çš„æ•°æ®")
    # çœŸå®ä¿¡å·
    t = np.linspace(0, 1, 100)
    signal = np.sin(2 * np.pi * t)
    
    # æ·»åŠ å™ªå£°
    noise_level = 0.5
    noisy_signal = signal + noise_level * np.random.randn(len(t))
    
    # åˆ›å»ºå»¶è¿Ÿç‰ˆæœ¬ä½œä¸ºé¢å¤–ç‰¹å¾
    X_signal = np.column_stack([
        noisy_signal,
        np.roll(noisy_signal, 1),  # å»¶è¿Ÿ1
        np.roll(noisy_signal, 2),  # å»¶è¿Ÿ2
    ])
    
    pca_signal = PCA()
    X_denoised = pca_signal.fit_transform(X_signal)
    
    print("ä¿¡å·æ•°æ®çš„è§£é‡Šæ–¹å·®æ¯”ä¾‹:", pca_signal.explained_variance_ratio_)
    
    # å¯è§†åŒ–å»å™ªæ•ˆæœ
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 3, 1)
    plt.plot(t, signal, 'b-', label='çœŸå®ä¿¡å·')
    plt.plot(t, noisy_signal, 'r-', alpha=0.7, label='å™ªå£°ä¿¡å·')
    plt.title('åŸå§‹æ•°æ®')
    plt.legend()
    
    plt.subplot(1, 3, 2)
    # ä½¿ç”¨ç¬¬ä¸€ä¸»æˆåˆ†é‡æ„
    X_reconstructed = pca_signal.inverse_transform(X_denoised[:, :1])
    plt.plot(t, signal, 'b-', label='çœŸå®ä¿¡å·')
    plt.plot(t, X_reconstructed[:, 0], 'g-', label='PCAå»å™ª')
    plt.title('PCAå»å™ª(ä»…ç¬¬ä¸€ä¸»æˆåˆ†)')
    plt.legend()
    
    plt.subplot(1, 3, 3)
    plt.bar(range(1, 4), pca_signal.explained_variance_ratio_)
    plt.title('å„ä¸»æˆåˆ†çš„è´¡çŒ®')
    plt.xlabel('ä¸»æˆåˆ†')
    plt.ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')
    
    plt.tight_layout()
    plt.show()

when_to_use_pca()
```

## å®æˆ˜æŠ€å·§å’Œæœ€ä½³å®è·µ

### 1. æ•°æ®é¢„å¤„ç†

```python
def pca_preprocessing_tips():
    """PCAçš„æ•°æ®é¢„å¤„ç†æŠ€å·§"""
    
    # åˆ›å»ºä¸åŒå°ºåº¦çš„æ•°æ®
    X_mixed_scale = np.column_stack([
        np.random.randn(100) * 1,      # æ ‡å‡†æ­£æ€åˆ†å¸ƒ
        np.random.randn(100) * 100,    # å¤§å°ºåº¦
        np.random.randn(100) * 0.01,   # å°å°ºåº¦
    ])
    
    print("åŸå§‹æ•°æ®çš„æ ‡å‡†å·®:")
    print(np.std(X_mixed_scale, axis=0))
    
    # ä¸æ ‡å‡†åŒ–çš„PCA
    pca_no_scaling = PCA()
    pca_no_scaling.fit(X_mixed_scale)
    
    # æ ‡å‡†åŒ–åçš„PCA
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_mixed_scale)
    
    pca_scaled = PCA()
    pca_scaled.fit(X_scaled)
    
    print("\nä¸æ ‡å‡†åŒ–çš„PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹:")
    print(pca_no_scaling.explained_variance_ratio_)
    
    print("\næ ‡å‡†åŒ–åçš„PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹:")
    print(pca_scaled.explained_variance_ratio_)
    
    print(f"\nç»“è®º: æ ‡å‡†åŒ–è®©æ–¹å·®åˆ†å¸ƒæ›´å‡åŒ€!")

pca_preprocessing_tips()
```

### 2. å¤„ç†ç¼ºå¤±å€¼

```python
def pca_with_missing_values():
    """å¤„ç†å«æœ‰ç¼ºå¤±å€¼çš„PCA"""
    
    # åˆ›å»ºå«ç¼ºå¤±å€¼çš„æ•°æ®
    X_complete = np.random.randn(100, 5)
    X_missing = X_complete.copy()
    
    # éšæœºæ·»åŠ ç¼ºå¤±å€¼
    missing_indices = np.random.choice(X_missing.size, size=int(0.1 * X_missing.size), replace=False)
    X_missing.flat[missing_indices] = np.nan
    
    print(f"ç¼ºå¤±å€¼æ¯”ä¾‹: {np.isnan(X_missing).sum() / X_missing.size:.1%}")
    
    # æ–¹æ³•1: åˆ é™¤å«ç¼ºå¤±å€¼çš„æ ·æœ¬
    X_dropna = X_missing[~np.isnan(X_missing).any(axis=1)]
    print(f"åˆ é™¤æ³•ä¿ç•™æ ·æœ¬æ•°: {len(X_dropna)}/{len(X_missing)}")
    
    # æ–¹æ³•2: å‡å€¼å¡«å……
    from sklearn.impute import SimpleImputer
    imputer = SimpleImputer(strategy='mean')
    X_imputed = imputer.fit_transform(X_missing)
    
    # æ¯”è¾ƒPCAç»“æœ
    pca_complete = PCA().fit(X_complete)
    pca_imputed = PCA().fit(X_imputed)
    
    print("\nå®Œæ•´æ•°æ®PCAè§£é‡Šæ–¹å·®:", pca_complete.explained_variance_ratio_[:3])
    print("å¡«å……æ•°æ®PCAè§£é‡Šæ–¹å·®:", pca_imputed.explained_variance_ratio_[:3])

pca_with_missing_values()
```

## æ€»ç»“ï¼šPCAçš„æ ¸å¿ƒæ€æƒ³

PCAå°±åƒæ˜¯ä¸€ä¸ª**æ™ºèƒ½çš„æ•°æ®æ‘„å½±å¸ˆ**ï¼š

1. ğŸ¯ **æ‰¾æœ€ä½³è§’åº¦**ï¼šå¯»æ‰¾æ•°æ®å˜åŒ–æœ€å¤§çš„æ–¹å‘
2. ğŸ“ **ä¿æŒå‚ç›´**ï¼šç¡®ä¿å„ä¸ªè§’åº¦äº’ä¸å¹²æ‰°ï¼ˆæ­£äº¤ï¼‰
3. ğŸ­ **çªå‡ºé‡ç‚¹**ï¼šæŒ‰é‡è¦æ€§æ’åºä¸»æˆåˆ†
4. âœ‚ï¸ **ç²¾ç®€è¡¨è¾¾**ï¼šç”¨æœ€å°‘çš„ç»´åº¦è¡¨è¾¾æœ€å¤šçš„ä¿¡æ¯
5. ğŸ”„ **å¯é€†å˜æ¢**ï¼šèƒ½å¤Ÿä»ä½ç»´æ¢å¤åˆ°é«˜ç»´ï¼ˆæœ‰æŸï¼‰

### è®°å¿†å£è¯€

- **æ‰¾æ–¹å‘**ï¼šæ‰¾åˆ°æ•°æ®çš„ä¸»è¦å˜åŒ–æ–¹å‘
- **æ’é¡ºåº**ï¼šæŒ‰æ–¹å·®å¤§å°æ’åˆ—ä¸»æˆåˆ†
- **é™ç»´åº¦**ï¼šé€‰æ‹©å‰å‡ ä¸ªä¸»è¦æˆåˆ†
- **ä¿ä¿¡æ¯**ï¼šå°½å¯èƒ½ä¿ç•™åŸå§‹ä¿¡æ¯

PCAä¸ä»…æ˜¯ä¸€ä¸ªå¼ºå¤§çš„é™ç»´å·¥å…·ï¼Œæ›´æ˜¯ç†è§£æ•°æ®ç»“æ„çš„çª—å£ã€‚å®ƒå‘Šè¯‰æˆ‘ä»¬ï¼š**å¤æ‚çš„é«˜ç»´æ•°æ®å¾€å¾€è•´å«ç€ç®€å•çš„ä½ç»´ç»“æ„**ï¼

---

**ä½œè€…**: meimeitou  
