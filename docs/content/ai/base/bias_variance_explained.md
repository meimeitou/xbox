+++
title = 'åå·®ä¸æ–¹å·®'
weight = 6
description = 'æ·±å…¥ç†è§£æœºå™¨å­¦ä¹ æ¨¡å‹çš„åå·®ä¸æ–¹å·®ï¼ŒæŒæ¡æ¨¡å‹æ€§èƒ½ä¼˜åŒ–çš„å…³é”®'
tags = ['æœºå™¨å­¦ä¹ ', 'åå·®', 'æ–¹å·®', 'æ¨¡å‹ä¼˜åŒ–']
+++

æœºå™¨å­¦ä¹ ä¸­çš„åå·®ä¸æ–¹å·®ï¼šæ·±å…¥ç†è§£æ¨¡å‹æ€§èƒ½çš„å…³é”®

- [å¼•è¨€](#å¼•è¨€)
- [æ ¸å¿ƒæ¦‚å¿µå®šä¹‰](#æ ¸å¿ƒæ¦‚å¿µå®šä¹‰)
  - [åå·®ï¼ˆBiasï¼‰](#åå·®bias)
  - [æ–¹å·®ï¼ˆVarianceï¼‰](#æ–¹å·®variance)
  - [ç›´è§‚ç†è§£](#ç›´è§‚ç†è§£)
- [åå·®-æ–¹å·®æƒè¡¡ï¼ˆBias-Variance Tradeoffï¼‰](#åå·®-æ–¹å·®æƒè¡¡bias-variance-tradeoff)
  - [æ•°å­¦åˆ†è§£](#æ•°å­¦åˆ†è§£)
  - [æƒè¡¡å…³ç³»](#æƒè¡¡å…³ç³»)
- [ä¸åŒæ¨¡å‹çš„åå·®-æ–¹å·®ç‰¹æ€§](#ä¸åŒæ¨¡å‹çš„åå·®-æ–¹å·®ç‰¹æ€§)
  - [1. çº¿æ€§å›å½’](#1-çº¿æ€§å›å½’)
  - [2. å†³ç­–æ ‘](#2-å†³ç­–æ ‘)
  - [3. k-è¿‘é‚»ç®—æ³•](#3-k-è¿‘é‚»ç®—æ³•)
- [å®é™…æ¡ˆä¾‹åˆ†æ](#å®é™…æ¡ˆä¾‹åˆ†æ)
  - [å¤šé¡¹å¼å›å½’çš„åå·®-æ–¹å·®åˆ†æ](#å¤šé¡¹å¼å›å½’çš„åå·®-æ–¹å·®åˆ†æ)
- [å¦‚ä½•è¯†åˆ«åå·®å’Œæ–¹å·®é—®é¢˜](#å¦‚ä½•è¯†åˆ«åå·®å’Œæ–¹å·®é—®é¢˜)
  - [1. å­¦ä¹ æ›²çº¿åˆ†æ](#1-å­¦ä¹ æ›²çº¿åˆ†æ)
  - [2. éªŒè¯æ›²çº¿åˆ†æ](#2-éªŒè¯æ›²çº¿åˆ†æ)
- [è§£å†³åå·®å’Œæ–¹å·®é—®é¢˜çš„ç­–ç•¥](#è§£å†³åå·®å’Œæ–¹å·®é—®é¢˜çš„ç­–ç•¥)
  - [è§£å†³é«˜åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰](#è§£å†³é«˜åå·®æ¬ æ‹Ÿåˆ)
  - [è§£å†³é«˜æ–¹å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰](#è§£å†³é«˜æ–¹å·®è¿‡æ‹Ÿåˆ)
- [é›†æˆæ–¹æ³•çš„åå·®-æ–¹å·®åˆ†æ](#é›†æˆæ–¹æ³•çš„åå·®-æ–¹å·®åˆ†æ)
  - [Baggingï¼šä¸»è¦å‡å°‘æ–¹å·®](#baggingä¸»è¦å‡å°‘æ–¹å·®)
  - [Boostingï¼šä¸»è¦å‡å°‘åå·®](#boostingä¸»è¦å‡å°‘åå·®)
- [å®é™…åº”ç”¨å»ºè®®](#å®é™…åº”ç”¨å»ºè®®)
  - [1. æ¨¡å‹é€‰æ‹©ç­–ç•¥](#1-æ¨¡å‹é€‰æ‹©ç­–ç•¥)
  - [2. è¶…å‚æ•°è°ƒä¼˜](#2-è¶…å‚æ•°è°ƒä¼˜)
- [æ€»ç»“å’Œæœ€ä½³å®è·µ](#æ€»ç»“å’Œæœ€ä½³å®è·µ)
  - [å…³é”®è¦ç‚¹](#å…³é”®è¦ç‚¹)
  - [å®è·µå»ºè®®](#å®è·µå»ºè®®)

## å¼•è¨€

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ**åå·®ï¼ˆBiasï¼‰** å’Œ **æ–¹å·®ï¼ˆVarianceï¼‰** æ˜¯è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„ä¸¤ä¸ªåŸºæœ¬æ¦‚å¿µã€‚ç†è§£è¿™ä¸¤ä¸ªæ¦‚å¿µå¯¹äºï¼š

- è¯Šæ–­æ¨¡å‹é—®é¢˜
- é€‰æ‹©åˆé€‚çš„ç®—æ³•
- è°ƒæ•´æ¨¡å‹å¤æ‚åº¦
- æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›

è‡³å…³é‡è¦ã€‚

## æ ¸å¿ƒæ¦‚å¿µå®šä¹‰

### åå·®ï¼ˆBiasï¼‰

**åå·®**æ˜¯æŒ‡æ¨¡å‹é¢„æµ‹å€¼çš„æœŸæœ›ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ã€‚å®ƒè¡¡é‡çš„æ˜¯æ¨¡å‹çš„**ç³»ç»Ÿæ€§é”™è¯¯**ã€‚

```
åå·® = E[fÌ‚(x)] - f(x)
```

å…¶ä¸­ï¼š

- `fÌ‚(x)` æ˜¯æ¨¡å‹çš„é¢„æµ‹
- `f(x)` æ˜¯çœŸå®å‡½æ•°
- `E[Â·]` è¡¨ç¤ºæœŸæœ›

### æ–¹å·®ï¼ˆVarianceï¼‰

**æ–¹å·®**æ˜¯æŒ‡åœ¨ä¸åŒè®­ç»ƒé›†ä¸Šè®­ç»ƒçš„æ¨¡å‹é¢„æµ‹ç»“æœçš„å˜åŒ–ç¨‹åº¦ã€‚å®ƒè¡¡é‡çš„æ˜¯æ¨¡å‹çš„**ä¸ç¨³å®šæ€§**ã€‚

```
æ–¹å·® = E[(fÌ‚(x) - E[fÌ‚(x)])Â²]
```

### ç›´è§‚ç†è§£

æƒ³è±¡å°„ç®­æ¯”èµ›ï¼š

- **ä½åå·®**ï¼šç®­çŸ¢å¹³å‡ä½ç½®æ¥è¿‘é¶å¿ƒ
- **é«˜åå·®**ï¼šç®­çŸ¢å¹³å‡ä½ç½®åç¦»é¶å¿ƒ
- **ä½æ–¹å·®**ï¼šç®­çŸ¢èšé›†ç´§å¯†
- **é«˜æ–¹å·®**ï¼šç®­çŸ¢æ•£å¸ƒå¾ˆå¹¿

```
é¶å¿ƒå›¾ç¤ºä¾‹ï¼š

ä½åå·®ï¼Œä½æ–¹å·®      é«˜åå·®ï¼Œä½æ–¹å·®
     ğŸ¯                 ğŸ¯
    â—â—â—              â—â—â—
    â—â—â—                â—â—â—

ä½åå·®ï¼Œé«˜æ–¹å·®      é«˜åå·®ï¼Œé«˜æ–¹å·®
     ğŸ¯                 ğŸ¯
   â— â—                â—   â—
  â—   â—              â—     â—
   â— â—                â—   â—
```

## åå·®-æ–¹å·®æƒè¡¡ï¼ˆBias-Variance Tradeoffï¼‰

### æ•°å­¦åˆ†è§£

å¯¹äºç»™å®šçš„æ•°æ®ç‚¹ï¼Œæ¨¡å‹çš„æœŸæœ›å‡æ–¹è¯¯å·®å¯ä»¥åˆ†è§£ä¸ºï¼š

```
MSE = åå·®Â² + æ–¹å·® + ä¸å¯çº¦è¯¯å·®
```

å…·ä½“æ¨å¯¼ï¼š

```python
# æœŸæœ›å‡æ–¹è¯¯å·®åˆ†è§£
E[(y - fÌ‚(x))Â²] = E[(y - f(x) + f(x) - fÌ‚(x))Â²]
                = E[(y - f(x))Â²] + E[(f(x) - fÌ‚(x))Â²] + 2E[(y - f(x))(f(x) - fÌ‚(x))]
                = ÏƒÂ² + [f(x) - E[fÌ‚(x)]]Â² + E[(E[fÌ‚(x)] - fÌ‚(x))Â²]
                = ä¸å¯çº¦è¯¯å·® + åå·®Â² + æ–¹å·®
```

### æƒè¡¡å…³ç³»

```python
import numpy as np
import matplotlib.pyplot as plt

# æ¨¡æ‹Ÿåå·®-æ–¹å·®æƒè¡¡
complexity = np.linspace(1, 20, 100)
bias_squared = 10 / complexity  # åå·®éšå¤æ‚åº¦é™ä½
variance = complexity * 0.1     # æ–¹å·®éšå¤æ‚åº¦å¢åŠ 
noise = np.ones_like(complexity) * 2  # ä¸å¯çº¦è¯¯å·®

total_error = bias_squared + variance + noise

plt.figure(figsize=(10, 6))
plt.plot(complexity, bias_squared, label='åå·®Â²', linewidth=2)
plt.plot(complexity, variance, label='æ–¹å·®', linewidth=2)
plt.plot(complexity, noise, label='ä¸å¯çº¦è¯¯å·®', linewidth=2, linestyle='--')
plt.plot(complexity, total_error, label='æ€»è¯¯å·®', linewidth=3, color='red')

plt.xlabel('æ¨¡å‹å¤æ‚åº¦')
plt.ylabel('è¯¯å·®')
plt.legend()
plt.title('åå·®-æ–¹å·®æƒè¡¡')
plt.grid(True, alpha=0.3)
plt.show()
```

## ä¸åŒæ¨¡å‹çš„åå·®-æ–¹å·®ç‰¹æ€§

### 1. çº¿æ€§å›å½’

```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# ç®€å•çº¿æ€§å›å½’ï¼šé«˜åå·®ï¼Œä½æ–¹å·®
model_simple = LinearRegression()

# ç‰¹ç‚¹ï¼š
# - åå·®ï¼šå¦‚æœçœŸå®å…³ç³»æ˜¯éçº¿æ€§ï¼Œåå·®è¾ƒé«˜
# - æ–¹å·®ï¼šæ¨¡å‹ç®€å•ï¼Œæ–¹å·®è¾ƒä½
# - é€‚ç”¨ï¼šæ•°æ®é‡å°ï¼ŒçœŸå®å…³ç³»æ¥è¿‘çº¿æ€§
```

### 2. å†³ç­–æ ‘

```python
from sklearn.tree import DecisionTreeRegressor

# æ·±åº¦å†³ç­–æ ‘ï¼šä½åå·®ï¼Œé«˜æ–¹å·®
model_tree = DecisionTreeRegressor(max_depth=None)

# æµ…å±‚å†³ç­–æ ‘ï¼šé«˜åå·®ï¼Œä½æ–¹å·®
model_tree_simple = DecisionTreeRegressor(max_depth=3)

# ç‰¹ç‚¹ï¼š
# - æ·±æ ‘ï¼šèƒ½æ‹Ÿåˆå¤æ‚å…³ç³»ï¼ˆä½åå·®ï¼‰ï¼Œä½†å¯¹æ•°æ®å˜åŒ–æ•æ„Ÿï¼ˆé«˜æ–¹å·®ï¼‰
# - æµ…æ ‘ï¼šæ‹Ÿåˆèƒ½åŠ›æœ‰é™ï¼ˆé«˜åå·®ï¼‰ï¼Œä½†ç¨³å®šæ€§å¥½ï¼ˆä½æ–¹å·®ï¼‰
```

### 3. k-è¿‘é‚»ç®—æ³•

```python
from sklearn.neighbors import KNeighborsRegressor

# kå€¼å°ï¼šä½åå·®ï¼Œé«˜æ–¹å·®
model_knn_complex = KNeighborsRegressor(n_neighbors=1)

# kå€¼å¤§ï¼šé«˜åå·®ï¼Œä½æ–¹å·®
model_knn_simple = KNeighborsRegressor(n_neighbors=20)

# ç‰¹ç‚¹ï¼š
# - kå°ï¼šå±€éƒ¨æ‹Ÿåˆå¥½ï¼Œä½†å™ªå£°æ•æ„Ÿ
# - kå¤§ï¼šå¹³æ»‘ä½†å¯èƒ½æ¬ æ‹Ÿåˆ
```

## å®é™…æ¡ˆä¾‹åˆ†æ

### å¤šé¡¹å¼å›å½’çš„åå·®-æ–¹å·®åˆ†æ

```python
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def generate_data(n_samples=100, noise=0.1):
    """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
    X = np.linspace(0, 1, n_samples).reshape(-1, 1)
    y = 1.5 * X.ravel() + np.sin(X.ravel() * 3 * np.pi) + np.random.normal(0, noise, n_samples)
    return X, y

def bias_variance_analysis(degree_range, n_experiments=100):
    """è¿›è¡Œåå·®-æ–¹å·®åˆ†æ"""
    X_test = np.linspace(0, 1, 50).reshape(-1, 1)
    y_true = 1.5 * X_test.ravel() + np.sin(X_test.ravel() * 3 * np.pi)
    
    results = []
    
    for degree in degree_range:
        predictions = []
        
        # å¤šæ¬¡å®éªŒ
        for _ in range(n_experiments):
            X_train, y_train = generate_data()
            
            # è®­ç»ƒæ¨¡å‹
            poly_features = PolynomialFeatures(degree=degree)
            X_train_poly = poly_features.fit_transform(X_train)
            X_test_poly = poly_features.transform(X_test)
            
            model = LinearRegression()
            model.fit(X_train_poly, y_train)
            
            y_pred = model.predict(X_test_poly)
            predictions.append(y_pred)
        
        predictions = np.array(predictions)
        
        # è®¡ç®—åå·®å’Œæ–¹å·®
        mean_pred = np.mean(predictions, axis=0)
        bias_squared = np.mean((mean_pred - y_true) ** 2)
        variance = np.mean(np.var(predictions, axis=0))
        
        results.append({
            'degree': degree,
            'bias_squared': bias_squared,
            'variance': variance,
            'total_error': bias_squared + variance
        })
    
    return results

# è¿è¡Œåˆ†æ
degree_range = range(1, 16)
results = bias_variance_analysis(degree_range)

# ç»˜åˆ¶ç»“æœ
degrees = [r['degree'] for r in results]
bias_squared = [r['bias_squared'] for r in results]
variances = [r['variance'] for r in results]
total_errors = [r['total_error'] for r in results]

plt.figure(figsize=(12, 8))
plt.plot(degrees, bias_squared, 'o-', label='åå·®Â²', linewidth=2)
plt.plot(degrees, variances, 's-', label='æ–¹å·®', linewidth=2)
plt.plot(degrees, total_errors, '^-', label='æ€»è¯¯å·®', linewidth=2)

plt.xlabel('å¤šé¡¹å¼æ¬¡æ•°')
plt.ylabel('è¯¯å·®')
plt.legend()
plt.title('å¤šé¡¹å¼å›å½’çš„åå·®-æ–¹å·®åˆ†æ')
plt.grid(True, alpha=0.3)
plt.show()

# æ‰¾åˆ°æœ€ä¼˜å¤æ‚åº¦
optimal_degree = degrees[np.argmin(total_errors)]
print(f"æœ€ä¼˜å¤šé¡¹å¼æ¬¡æ•°: {optimal_degree}")
```

## å¦‚ä½•è¯†åˆ«åå·®å’Œæ–¹å·®é—®é¢˜

### 1. å­¦ä¹ æ›²çº¿åˆ†æ

```python
from sklearn.model_selection import learning_curve

def plot_learning_curves(model, X, y, title):
    """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=5, n_jobs=-1, 
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='neg_mean_squared_error'
    )
    
    train_mean = -train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='è®­ç»ƒè¯¯å·®')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
    
    plt.plot(train_sizes, val_mean, 'o-', color='red', label='éªŒè¯è¯¯å·®')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
    
    plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
    plt.ylabel('å‡æ–¹è¯¯å·®')
    plt.legend()
    plt.title(f'{title} - å­¦ä¹ æ›²çº¿')
    plt.grid(True, alpha=0.3)
    plt.show()

# è¯Šæ–­æŒ‡å—ï¼š
# 1. é«˜åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰ï¼š
#    - è®­ç»ƒè¯¯å·®å’ŒéªŒè¯è¯¯å·®éƒ½å¾ˆé«˜
#    - ä¸¤è€…ä¹‹é—´å·®è·è¾ƒå°
#    - å¢åŠ è®­ç»ƒæ•°æ®å¸®åŠ©ä¸å¤§

# 2. é«˜æ–¹å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼š
#    - è®­ç»ƒè¯¯å·®å¾ˆä½ï¼ŒéªŒè¯è¯¯å·®å¾ˆé«˜
#    - ä¸¤è€…ä¹‹é—´å·®è·å¾ˆå¤§
#    - å¢åŠ è®­ç»ƒæ•°æ®å¯èƒ½æœ‰å¸®åŠ©
```

### 2. éªŒè¯æ›²çº¿åˆ†æ

```python
from sklearn.model_selection import validation_curve

def plot_validation_curve(model, X, y, param_name, param_range, title):
    """ç»˜åˆ¶éªŒè¯æ›²çº¿"""
    train_scores, val_scores = validation_curve(
        model, X, y, param_name=param_name, param_range=param_range,
        cv=5, scoring='neg_mean_squared_error'
    )
    
    train_mean = -train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.semilogx(param_range, train_mean, 'o-', color='blue', label='è®­ç»ƒè¯¯å·®')
    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
    
    plt.semilogx(param_range, val_mean, 'o-', color='red', label='éªŒè¯è¯¯å·®')
    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
    
    plt.xlabel(param_name)
    plt.ylabel('å‡æ–¹è¯¯å·®')
    plt.legend()
    plt.title(f'{title} - éªŒè¯æ›²çº¿')
    plt.grid(True, alpha=0.3)
    plt.show()
```

## è§£å†³åå·®å’Œæ–¹å·®é—®é¢˜çš„ç­–ç•¥

### è§£å†³é«˜åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰

```python
# 1. å¢åŠ æ¨¡å‹å¤æ‚åº¦
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor

# æ›´å¤æ‚çš„æ¨¡å‹
complex_models = [
    RandomForestRegressor(n_estimators=100),
    MLPRegressor(hidden_layer_sizes=(100, 50)),
]

# 2. å¢åŠ ç‰¹å¾
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=3, include_bias=False)
X_poly = poly.fit_transform(X)

# 3. å‡å°‘æ­£åˆ™åŒ–
from sklearn.linear_model import Ridge

model_less_reg = Ridge(alpha=0.01)  # å‡å°‘æ­£åˆ™åŒ–å‚æ•°
```

### è§£å†³é«˜æ–¹å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰

```python
# 1. å¢åŠ è®­ç»ƒæ•°æ®
# - æ”¶é›†æ›´å¤šæ•°æ®
# - æ•°æ®å¢å¼º
# - åˆæˆæ•°æ®

# 2. ç®€åŒ–æ¨¡å‹
from sklearn.tree import DecisionTreeRegressor

simple_tree = DecisionTreeRegressor(max_depth=5, min_samples_split=20)

# 3. æ­£åˆ™åŒ–
from sklearn.linear_model import Lasso, Ridge

regularized_models = [
    Ridge(alpha=1.0),
    Lasso(alpha=0.1),
]

# 4. é›†æˆæ–¹æ³•
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor

ensemble_models = [
    BaggingRegressor(n_estimators=100),
    RandomForestRegressor(n_estimators=100),
]

# 5. äº¤å‰éªŒè¯
from sklearn.model_selection import cross_val_score

def evaluate_model(model, X, y):
    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
    return -scores.mean(), scores.std()
```

## é›†æˆæ–¹æ³•çš„åå·®-æ–¹å·®åˆ†æ

### Baggingï¼šä¸»è¦å‡å°‘æ–¹å·®

```python
from sklearn.ensemble import BaggingRegressor

# Baggingé€šè¿‡å¹³å‡å¤šä¸ªæ¨¡å‹çš„é¢„æµ‹æ¥å‡å°‘æ–¹å·®
bagging = BaggingRegressor(
    base_estimator=DecisionTreeRegressor(max_depth=None),
    n_estimators=100,
    random_state=42
)

# åŸç†ï¼š
# - åœ¨ä¸åŒçš„æ•°æ®å­é›†ä¸Šè®­ç»ƒå¤šä¸ªæ¨¡å‹
# - å¹³å‡é¢„æµ‹ç»“æœ
# - å‡å°‘æ–¹å·®ï¼Œåå·®åŸºæœ¬ä¸å˜
```

### Boostingï¼šä¸»è¦å‡å°‘åå·®

```python
from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor

# Boostingé€šè¿‡é¡ºåºè®­ç»ƒæ¨¡å‹æ¥å‡å°‘åå·®
boosting_models = [
    AdaBoostRegressor(n_estimators=100),
    GradientBoostingRegressor(n_estimators=100),
]

# åŸç†ï¼š
# - é¡ºåºè®­ç»ƒå¤šä¸ªå¼±å­¦ä¹ å™¨
# - æ¯ä¸ªæ–°æ¨¡å‹å…³æ³¨å‰é¢æ¨¡å‹çš„é”™è¯¯
# - ä¸»è¦å‡å°‘åå·®ï¼Œå¯èƒ½å¢åŠ æ–¹å·®
```

## å®é™…åº”ç”¨å»ºè®®

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

```python
def model_selection_strategy(X, y):
    """æ¨¡å‹é€‰æ‹©ç­–ç•¥"""
    
    # æ­¥éª¤1ï¼šç®€å•æ¨¡å‹å¼€å§‹
    simple_models = [
        LinearRegression(),
        DecisionTreeRegressor(max_depth=5)
    ]
    
    # æ­¥éª¤2ï¼šè¯„ä¼°åå·®-æ–¹å·®
    for model in simple_models:
        # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
        plot_learning_curves(model, X, y, model.__class__.__name__)
    
    # æ­¥éª¤3ï¼šæ ¹æ®è¯Šæ–­è°ƒæ•´
    # å¦‚æœé«˜åå·®ï¼šå¢åŠ å¤æ‚åº¦
    # å¦‚æœé«˜æ–¹å·®ï¼šç®€åŒ–æ¨¡å‹æˆ–æ­£åˆ™åŒ–
    
    # æ­¥éª¤4ï¼šé›†æˆæ–¹æ³•
    if high_variance_detected:
        return BaggingRegressor()
    elif high_bias_detected:
        return GradientBoostingRegressor()
    else:
        return best_simple_model
```

### 2. è¶…å‚æ•°è°ƒä¼˜

```python
from sklearn.model_selection import GridSearchCV

def tune_bias_variance_tradeoff(model, param_grid, X, y):
    """è°ƒä¼˜åå·®-æ–¹å·®æƒè¡¡"""
    
    grid_search = GridSearchCV(
        model, param_grid, 
        cv=5, 
        scoring='neg_mean_squared_error',
        return_train_score=True
    )
    
    grid_search.fit(X, y)
    
    # åˆ†æç»“æœ
    results = grid_search.cv_results_
    
    # æ‰¾åˆ°æœ€ä½³åå·®-æ–¹å·®æƒè¡¡ç‚¹
    best_params = grid_search.best_params_
    
    return grid_search.best_estimator_, best_params
```

## æ€»ç»“å’Œæœ€ä½³å®è·µ

### å…³é”®è¦ç‚¹

1. **åå·®-æ–¹å·®æƒè¡¡æ˜¯æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µ**
   - åå·®ï¼šæ¨¡å‹çš„ç³»ç»Ÿæ€§é”™è¯¯
   - æ–¹å·®ï¼šæ¨¡å‹çš„ä¸ç¨³å®šæ€§
   - æ€»è¯¯å·® = åå·®Â² + æ–¹å·® + ä¸å¯çº¦è¯¯å·®

2. **è¯Šæ–­æ–¹æ³•**
   - å­¦ä¹ æ›²çº¿ï¼šè¯†åˆ«æ¬ æ‹Ÿåˆvsè¿‡æ‹Ÿåˆ
   - éªŒè¯æ›²çº¿ï¼šæ‰¾åˆ°æœ€ä½³å¤æ‚åº¦
   - äº¤å‰éªŒè¯ï¼šè¯„ä¼°æ¨¡å‹ç¨³å®šæ€§

3. **è§£å†³ç­–ç•¥**
   - é«˜åå·®ï¼šå¢åŠ å¤æ‚åº¦ã€ç‰¹å¾å·¥ç¨‹
   - é«˜æ–¹å·®ï¼šç®€åŒ–æ¨¡å‹ã€æ­£åˆ™åŒ–ã€å¢åŠ æ•°æ®

### å®è·µå»ºè®®

```python
# å®Œæ•´çš„åå·®-æ–¹å·®åˆ†ææµç¨‹
def complete_bias_variance_analysis(X, y):
    """å®Œæ•´çš„åå·®-æ–¹å·®åˆ†ææµç¨‹"""
    
    # 1. æ•°æ®åˆ’åˆ†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # 2. åŸºçº¿æ¨¡å‹
    baseline = LinearRegression()
    baseline.fit(X_train, y_train)
    
    # 3. å­¦ä¹ æ›²çº¿åˆ†æ
    plot_learning_curves(baseline, X_train, y_train, "Baseline")
    
    # 4. æ¨¡å‹å¤æ‚åº¦åˆ†æ
    models = [
        ('Linear', LinearRegression()),
        ('Tree-3', DecisionTreeRegressor(max_depth=3)),
        ('Tree-10', DecisionTreeRegressor(max_depth=10)),
        ('RF', RandomForestRegressor(n_estimators=100)),
    ]
    
    for name, model in models:
        model.fit(X_train, y_train)
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)
        print(f"{name}: Train={train_score:.3f}, Test={test_score:.3f}")
    
    # 5. é€‰æ‹©æœ€ä½³æ¨¡å‹
    # åŸºäºåå·®-æ–¹å·®æƒè¡¡åŸåˆ™
    
    return best_model
```

ç†è§£åå·®å’Œæ–¹å·®æ˜¯æˆä¸ºä¼˜ç§€æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆçš„å¿…å¤‡æŠ€èƒ½ã€‚é€šè¿‡æŒæ¡è¿™äº›æ¦‚å¿µï¼Œæ‚¨å¯ä»¥ï¼š

- æ›´å¥½åœ°è¯Šæ–­æ¨¡å‹é—®é¢˜
- é€‰æ‹©åˆé€‚çš„ç®—æ³•å’Œå‚æ•°
- æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
- é¿å…å¸¸è§çš„å»ºæ¨¡é™·é˜±

è®°ä½ï¼š**æœ€å¥½çš„æ¨¡å‹ä¸æ˜¯è®­ç»ƒè¯¯å·®æœ€å°çš„ï¼Œè€Œæ˜¯åœ¨åå·®å’Œæ–¹å·®ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡çš„æ¨¡å‹**ã€‚

---

ä½œè€…ï¼š meimeitou
