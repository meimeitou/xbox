+++
title = 'éƒ¨ç½²æœ¬åœ°LLM'
description = 'ä½¿ç”¨Dockeréƒ¨ç½²æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼Œæ‰“é€ ä½ çš„ç§äººAIåŠ©æ‰‹ã€‚'
tags = ['æœ¬åœ°éƒ¨ç½²', 'å¤§è¯­è¨€æ¨¡å‹', 'Docker', 'AIåŠ©æ‰‹']
categories = ['äººå·¥æ™ºèƒ½', 'å¤§è¯­è¨€æ¨¡å‹']
+++

æœ¬åœ°å¤§æ¨¡å‹Dockeréƒ¨ç½²æŒ‡å—ï¼šæ‰“é€ ä½ çš„ç§äººAIåŠ©æ‰‹

- [å‰è¨€](#å‰è¨€)
- [ä¸ºä»€ä¹ˆé€‰æ‹©æœ¬åœ°éƒ¨ç½²ï¼Ÿ](#ä¸ºä»€ä¹ˆé€‰æ‹©æœ¬åœ°éƒ¨ç½²)
  - [ğŸ”’ éšç§ä¿æŠ¤](#-éšç§ä¿æŠ¤)
  - [ğŸ’° æˆæœ¬ä¼˜åŠ¿](#-æˆæœ¬ä¼˜åŠ¿)
  - [âš¡ è‡ªä¸»å¯æ§](#-è‡ªä¸»å¯æ§)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
  - [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
    - [æœ€ä½é…ç½®](#æœ€ä½é…ç½®)
    - [æ¨èé…ç½®](#æ¨èé…ç½®)
  - [Dockerç¯å¢ƒéªŒè¯](#dockerç¯å¢ƒéªŒè¯)
- [æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨Ollamaè¿›è¡Œéƒ¨ç½²](#æ–¹æ¡ˆä¸€ä½¿ç”¨ollamaè¿›è¡Œéƒ¨ç½²)
  - [ğŸš€ å¿«é€Ÿéƒ¨ç½²](#-å¿«é€Ÿéƒ¨ç½²)
    - [1. æ‹‰å–Ollama Dockeré•œåƒ](#1-æ‹‰å–ollama-dockeré•œåƒ)
    - [2. å¯åŠ¨OllamaæœåŠ¡](#2-å¯åŠ¨ollamaæœåŠ¡)
    - [3. éªŒè¯æœåŠ¡çŠ¶æ€](#3-éªŒè¯æœåŠ¡çŠ¶æ€)
  - [ğŸ“¥ ä¸‹è½½å’Œç®¡ç†æ¨¡å‹](#-ä¸‹è½½å’Œç®¡ç†æ¨¡å‹)
    - [è¿›å…¥å®¹å™¨ç¯å¢ƒ](#è¿›å…¥å®¹å™¨ç¯å¢ƒ)
    - [ä¸‹è½½å¸¸ç”¨æ¨¡å‹](#ä¸‹è½½å¸¸ç”¨æ¨¡å‹)
    - [æ¨¡å‹ä¿¡æ¯å±•ç¤º](#æ¨¡å‹ä¿¡æ¯å±•ç¤º)
  - [ğŸ’¬ å‘½ä»¤è¡Œäº¤äº’ä½“éªŒ](#-å‘½ä»¤è¡Œäº¤äº’ä½“éªŒ)
    - [1. åŸºç¡€å¯¹è¯](#1-åŸºç¡€å¯¹è¯)
  - [2.2 åŸºäºZookeeperçš„åˆ†å¸ƒå¼é”](#22-åŸºäºzookeeperçš„åˆ†å¸ƒå¼é”)
- [3. æ¨èå®ç°æ–¹æ¡ˆ](#3-æ¨èå®ç°æ–¹æ¡ˆ)
  - [3.1 Redis + Redissonæ–¹æ¡ˆï¼ˆæ¨èï¼‰](#31-redis--redissonæ–¹æ¡ˆæ¨è)
  - [3.2 æ³¨è§£å¼åˆ†å¸ƒå¼é”](#32-æ³¨è§£å¼åˆ†å¸ƒå¼é”)
- [4. æœ€ä½³å®è·µ](#4-æœ€ä½³å®è·µ)
  - [4.1 é”çš„ç²’åº¦æ§åˆ¶](#41-é”çš„ç²’åº¦æ§åˆ¶)
  - [4.2 å¼‚å¸¸å¤„ç†](#42-å¼‚å¸¸å¤„ç†)
    - [2. åˆ›å»ºDocker Composeé…ç½®](#2-åˆ›å»ºdocker-composeé…ç½®)
    - [3. å¯åŠ¨æœåŠ¡](#3-å¯åŠ¨æœåŠ¡)
  - [ğŸ“¥ æ¨¡å‹ä¸‹è½½å’Œç®¡ç†](#-æ¨¡å‹ä¸‹è½½å’Œç®¡ç†)
    - [ä¸‹è½½æ¨¡å‹æ–‡ä»¶](#ä¸‹è½½æ¨¡å‹æ–‡ä»¶)
    - [è‡ªåŠ¨ä¸‹è½½è„šæœ¬](#è‡ªåŠ¨ä¸‹è½½è„šæœ¬)
  - [ğŸ”§ APIè°ƒç”¨ç¤ºä¾‹](#-apiè°ƒç”¨ç¤ºä¾‹)
    - [Pythonå®¢æˆ·ç«¯](#pythonå®¢æˆ·ç«¯)
    - [ä½¿ç”¨å®¢æˆ·ç«¯](#ä½¿ç”¨å®¢æˆ·ç«¯)
    - [2. åˆ›å»ºDocker Composeé…ç½®](#2-åˆ›å»ºdocker-composeé…ç½®-1)
    - [3. ä¸‹è½½æ¨¡å‹é…ç½®](#3-ä¸‹è½½æ¨¡å‹é…ç½®)
    - [4. å¯åŠ¨æœåŠ¡](#4-å¯åŠ¨æœåŠ¡)
  - [ğŸ’¬ OpenAIå…¼å®¹å®¢æˆ·ç«¯](#-openaiå…¼å®¹å®¢æˆ·ç«¯)
    - [Pythonå®¢æˆ·ç«¯](#pythonå®¢æˆ·ç«¯-1)
    - [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [2. è£…é¥°å™¨æ–¹å¼](#2-è£…é¥°å™¨æ–¹å¼)
- [3. å…ƒç±»æ–¹å¼](#3-å…ƒç±»æ–¹å¼)
  - [ğŸ¯ æ€§èƒ½è°ƒä¼˜å»ºè®®](#-æ€§èƒ½è°ƒä¼˜å»ºè®®)
    - [1. å†…å­˜ä¼˜åŒ–](#1-å†…å­˜ä¼˜åŒ–)
    - [2. Dockerä¼˜åŒ–](#2-dockerä¼˜åŒ–)
    - [3. æ¨¡å‹é€‰æ‹©ç­–ç•¥](#3-æ¨¡å‹é€‰æ‹©ç­–ç•¥)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
  - [ğŸ”§ å¸¸è§é—®é¢˜è§£å†³](#-å¸¸è§é—®é¢˜è§£å†³)
    - [1. å†…å­˜ä¸è¶³](#1-å†…å­˜ä¸è¶³)
    - [2. GPUä¸è¢«è¯†åˆ«](#2-gpuä¸è¢«è¯†åˆ«)
    - [3. æ¨¡å‹ä¸‹è½½å¤±è´¥](#3-æ¨¡å‹ä¸‹è½½å¤±è´¥)
  - [ğŸ› ï¸ è¯Šæ–­è„šæœ¬](#ï¸-è¯Šæ–­è„šæœ¬)
- [æ€»ç»“ä¸å±•æœ›](#æ€»ç»“ä¸å±•æœ›)
  - [ğŸ¯ æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“](#-æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“)
  - [ğŸš€ æœ€ä½³å®è·µå»ºè®®](#-æœ€ä½³å®è·µå»ºè®®)
  - [ğŸ’¡ æœªæ¥å‘å±•æ–¹å‘](#-æœªæ¥å‘å±•æ–¹å‘)
- [ç›¸å…³èµ„æº](#ç›¸å…³èµ„æº)
  - [ğŸ“š å®˜æ–¹æ–‡æ¡£](#-å®˜æ–¹æ–‡æ¡£)
  - [ğŸ”— æœ‰ç”¨é“¾æ¥](#-æœ‰ç”¨é“¾æ¥)
  - [ğŸ› ï¸ å·¥å…·æ¨è](#ï¸-å·¥å…·æ¨è)

## å‰è¨€

åœ¨AIæŠ€æœ¯æ—¥ç›Šæ™®åŠçš„ä»Šå¤©ï¼Œæ‹¥æœ‰ä¸€ä¸ªç§äººçš„AIåŠ©æ‰‹ä¸å†æ˜¯é¥ä¸å¯åŠçš„æ¢¦æƒ³ã€‚é€šè¿‡Dockeréƒ¨ç½²æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ï¼Œäº«å—AIå¸¦æ¥çš„ä¾¿åˆ©ã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨Dockeråœ¨æœ¬åœ°éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶é€šè¿‡å‘½ä»¤è¡Œè¿›è¡Œäº¤äº’ï¼Œè®©ä½ è½»æ¾æ‹¥æœ‰å±äºè‡ªå·±çš„AIå¯¹è¯ä¼™ä¼´ã€‚

## ä¸ºä»€ä¹ˆé€‰æ‹©æœ¬åœ°éƒ¨ç½²ï¼Ÿ

### ğŸ”’ éšç§ä¿æŠ¤

- **æ•°æ®ä¸å‡ºæœ¬åœ°**ï¼šæ‰€æœ‰å¯¹è¯å†…å®¹éƒ½åœ¨æœ¬åœ°å¤„ç†
- **æ— ç½‘ç»œä¾èµ–**ï¼šç¦»çº¿ç¯å¢ƒä¸‹ä¹Ÿèƒ½æ­£å¸¸ä½¿ç”¨
- **å®Œå…¨æ§åˆ¶**ï¼šå¯¹æ•°æ®å¤„ç†è¿‡ç¨‹æœ‰å®Œå…¨çš„æŒæ§æƒ

### ğŸ’° æˆæœ¬ä¼˜åŠ¿

- **é›¶APIè´¹ç”¨**ï¼šæ— éœ€æ”¯ä»˜äº‘ç«¯APIè°ƒç”¨è´¹ç”¨
- **é•¿æœŸç»æµ**ï¼šä¸€æ¬¡éƒ¨ç½²ï¼Œé•¿æœŸä½¿ç”¨
- **èµ„æºè‡ªæ§**ï¼šæ ¹æ®éœ€æ±‚çµæ´»è°ƒæ•´èµ„æºåˆ†é…

### âš¡ è‡ªä¸»å¯æ§

- **å®šåˆ¶åŒ–é…ç½®**ï¼šå¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´æ¨¡å‹å‚æ•°
- **ç‰ˆæœ¬æ§åˆ¶**ï¼šå¯ä»¥é€‰æ‹©å’Œå›ºå®šç‰¹å®šçš„æ¨¡å‹ç‰ˆæœ¬
- **æœåŠ¡ç¨³å®š**ï¼šä¸å—å¤–éƒ¨æœåŠ¡çŠ¶æ€å½±å“

## ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚

#### æœ€ä½é…ç½®

```
CPU: 4æ ¸å¿ƒä»¥ä¸Š
å†…å­˜: 16GB RAM
å­˜å‚¨: 50GBå¯ç”¨ç©ºé—´
GPU: å¯é€‰ï¼Œä½†æ¨èNVIDIA GPU
```

#### æ¨èé…ç½®

```
CPU: 8æ ¸å¿ƒä»¥ä¸Š (Intel i7/AMD Ryzen 7)
å†…å­˜: 32GB RAM
å­˜å‚¨: 100GB+ SSD
GPU: NVIDIA RTX 4070ä»¥ä¸Š (12GB+ VRAM)
```

### Dockerç¯å¢ƒéªŒè¯

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç¡®è®¤Dockerç¯å¢ƒæ˜¯å¦æ­£å¸¸ï¼š

```bash
# æ£€æŸ¥Dockerç‰ˆæœ¬
docker --version
# é¢„æœŸè¾“å‡º: Docker version 24.0.x, build xxxxx

# æ£€æŸ¥DockeræœåŠ¡çŠ¶æ€
docker info
# ç¡®è®¤Docker daemonæ­£åœ¨è¿è¡Œ

# æµ‹è¯•DockeråŠŸèƒ½
docker run hello-world
# åº”è¯¥çœ‹åˆ° "Hello from Docker!" æ¶ˆæ¯
```

å¦‚æœæœ‰NVIDIA GPUï¼Œè¿˜éœ€è¦éªŒè¯Dockerå¯¹GPUçš„æ”¯æŒï¼š

```bash
# æ£€æŸ¥NVIDIA Dockeræ”¯æŒ
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi
```

## æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨Ollamaè¿›è¡Œéƒ¨ç½²

Ollamaæ˜¯ç›®å‰æœ€å—æ¬¢è¿çš„æœ¬åœ°å¤§æ¨¡å‹ç®¡ç†å·¥å…·ï¼Œæä¾›äº†ç®€æ´çš„Dockeréƒ¨ç½²æ–¹æ¡ˆã€‚

### ğŸš€ å¿«é€Ÿéƒ¨ç½²

#### 1. æ‹‰å–Ollama Dockeré•œåƒ

```bash
# æ‹‰å–æœ€æ–°çš„Ollamaé•œåƒ
docker pull ollama/ollama:latest

# æŸ¥çœ‹é•œåƒä¿¡æ¯
docker images | grep ollama
```

#### 2. å¯åŠ¨OllamaæœåŠ¡

```bash
# CPUç‰ˆæœ¬éƒ¨ç½²
docker run -d \
  --name ollama-server \
  -p 11434:11434 \
  -v ollama-data:/root/.ollama \
  -v /tmp:/tmp \
  --restart unless-stopped \
  ollama/ollama

# GPUç‰ˆæœ¬éƒ¨ç½²ï¼ˆæ¨èï¼‰
docker run -d \
  --name ollama-server \
  --gpus all \
  -p 11434:11434 \
  -v ollama-data:/root/.ollama \
  -v /tmp:/tmp \
  --restart unless-stopped \
  ollama/ollama
```

#### 3. éªŒè¯æœåŠ¡çŠ¶æ€

```bash
# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker ps | grep ollama

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
docker logs ollama-server

# æµ‹è¯•APIè¿æ¥
curl http://localhost:11434/api/version
```

### ğŸ“¥ ä¸‹è½½å’Œç®¡ç†æ¨¡å‹

#### è¿›å…¥å®¹å™¨ç¯å¢ƒ

```bash
# è¿›å…¥Ollamaå®¹å™¨
docker exec -it ollama-server bash
```

#### ä¸‹è½½å¸¸ç”¨æ¨¡å‹

```bash
# åœ¨å®¹å™¨å†…æ‰§è¡Œä»¥ä¸‹å‘½ä»¤

# ä¸‹è½½Llama 3.1 8Bæ¨¡å‹ï¼ˆæ¨èå…¥é—¨ï¼‰
ollama pull llama3.1:8b

# ä¸‹è½½DeepSeek Coderæ¨¡å‹ï¼ˆä»£ç ç”Ÿæˆä¸“ç”¨ï¼‰
ollama pull deepseek-coder:6.7b

# ä¸‹è½½Qwen2.5æ¨¡å‹ï¼ˆä¸­æ–‡å‹å¥½ï¼‰
ollama pull qwen2.5:7b

# ä¸‹è½½Gemma2æ¨¡å‹ï¼ˆGoogleå¼€æºï¼‰
ollama pull gemma2:9b

# æŸ¥çœ‹å·²ä¸‹è½½çš„æ¨¡å‹
ollama list
```

#### æ¨¡å‹ä¿¡æ¯å±•ç¤º

```bash
# æŸ¥çœ‹æ¨¡å‹è¯¦ç»†ä¿¡æ¯
$ ollama list
NAME                    ID              SIZE    MODIFIED        
llama3.1:8b            42182c40c896    4.7GB   2 minutes ago   
deepseek-coder:6.7b    8b09afb70a0e    3.8GB   5 minutes ago   
qwen2.5:7b             845dbda0c5b4    4.3GB   8 minutes ago   
```

### ğŸ’¬ å‘½ä»¤è¡Œäº¤äº’ä½“éªŒ

#### 1. åŸºç¡€å¯¹è¯

```bash
# å¯åŠ¨ä¸Llama3.1çš„å¯¹è¯
docker exec -it ollama-server ollama run llama3.1:8b

# äº¤äº’ç¤ºä¾‹
>>> ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±
æˆ‘æ˜¯Llama 3.1ï¼Œä¸€ä¸ªç”±Metaå¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š

1. å›ç­”å„ç§é—®é¢˜
2. ååŠ©åˆ›ä½œå’Œå†™ä½œ
3. è¿›è¡Œä»£ç ç¼–ç¨‹
4. ç¿»è¯‘å’Œè¯­è¨€å­¦ä¹ 
5. åˆ†æå’Œæ€»ç»“æ–‡æœ¬

æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ

>>> ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°

å½“ç„¶ï¼è¿™é‡Œæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ–æ³¢é‚£å¥‘æ•°åˆ—è®¡ç®—å‡½æ•°ï¼š

```python
def fibonacci(n):
    """
    è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹
    ä½¿ç”¨åŠ¨æ€è§„åˆ’æ–¹æ³•ï¼Œæ—¶é—´å¤æ‚åº¦O(n)ï¼Œç©ºé—´å¤æ‚åº¦O(1)
    """
    if n <= 0:
        return 0
    elif n == 1:
        return 1
    
    a, b = 0, 1
    for i in range(2, n + 1):
        a, b = b, a + b
    
    return b

# ä½¿ç”¨ç¤ºä¾‹
for i in range(10):
    print(f"F({i}) = {fibonacci(i)}")
```

è¿™ä¸ªå‡½æ•°çš„ä¼˜ç‚¹ï¼š

- æ—¶é—´å¤æ‚åº¦ï¼šO(n)
- ç©ºé—´å¤æ‚åº¦ï¼šO(1)
- é¿å…äº†é€’å½’å¯¼è‡´çš„é‡å¤è®¡ç®—

>>> /bye

```

#### 2. ä»£ç ä¸“å®¶å¯¹è¯
```bash
# ä½¿ç”¨DeepSeek Coderè¿›è¡Œä»£ç ç›¸å…³å¯¹è¯
docker exec -it ollama-server ollama run deepseek-coder:6.7b

# ä»£ç ç”Ÿæˆç¤ºä¾‹
>>> å†™ä¸€ä¸ªFastAPIçš„ç”¨æˆ·è®¤è¯ä¸­é—´ä»¶

```python
from fastapi import FastAPI, HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.base import BaseHTTPMiddleware
from fastapi.requests import Request
from fastapi.responses import JSONResponse
import jwt
from datetime import datetime, timedelta
import logging

# é…ç½®
SECRET_KEY = "your-secret-key-here"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

# å®‰å…¨æ–¹æ¡ˆ
security = HTTPBearer()

class AuthenticationMiddleware(BaseHTTPMiddleware):
    """ç”¨æˆ·è®¤è¯ä¸­é—´ä»¶"""
    
    def __init__(self, app, exclude_paths=None):
        super().__init__(app)
        self.exclude_paths = exclude_paths or ["/docs", "/openapi.json", "/login", "/register"]
    
    async def dispatch(self, request: Request, call_next):
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ’é™¤è·¯å¾„
        if request.url.path in self.exclude_paths:
            return await call_next(request)
        
        # æ£€æŸ¥Authorizationå¤´
        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith("Bearer "):
            return JSONResponse(
                status_code=status.HTTP_401_UNAUTHORIZED,
                content={"detail": "Missing or invalid authorization header"}
            )
        
        # æå–token
        token = auth_header.split(" ")[1]
        
        try:
            # éªŒè¯JWT token
            payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
            user_id = payload.get("sub")
            
            if user_id is None:
                raise HTTPException(status_code=401, detail="Invalid token")
            
            # å°†ç”¨æˆ·ä¿¡æ¯æ·»åŠ åˆ°è¯·æ±‚çŠ¶æ€
            request.state.user_id = user_id
            request.state.user_email = payload.get("email")
            
        except jwt.ExpiredSignatureError:
            return JSONResponse(
                status_code=status.HTTP_401_UNAUTHORIZED,
                content={"detail": "Token has expired"}
            )
        except jwt.JWTError:
            return JSONResponse(
                status_code=status.HTTP_401_UNAUTHORIZED,
                content={"detail": "Invalid token"}
            )
        
        return await call_next(request)

# ä½¿ç”¨ç¤ºä¾‹
app = FastAPI()

# æ·»åŠ è®¤è¯ä¸­é—´ä»¶
app.add_middleware(
    AuthenticationMiddleware,
    exclude_paths=["/docs", "/openapi.json", "/login", "/register", "/health"]
)

@app.get("/protected")
async def protected_route(request: Request):
    return {
        "message": "This is a protected route",
        "user_id": request.state.user_id,
        "user_email": request.state.user_email
    }
```

>>> è¿™ä¸ªä¸­é—´ä»¶æœ‰ä»€ä¹ˆæ”¹è¿›å»ºè®®å—ï¼Ÿ

è¿™ä¸ªè®¤è¯ä¸­é—´ä»¶å·²ç»å¾ˆä¸é”™äº†ï¼Œä½†è¿˜æœ‰ä¸€äº›æ”¹è¿›ç©ºé—´ï¼š

1. **ç¼“å­˜ä¼˜åŒ–**ï¼š

```python
import redis
from functools import lru_cache

class AuthenticationMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, exclude_paths=None, redis_client=None):
        super().__init__(app)
        self.exclude_paths = exclude_paths or []
        self.redis_client = redis_client
        
    @lru_cache(maxsize=1000)
    def is_excluded_path(self, path: str) -> bool:
        """ç¼“å­˜è·¯å¾„æ£€æŸ¥ç»“æœ"""
        return any(path.startswith(excluded) for excluded in self.exclude_paths)
```

2. **ç”¨æˆ·ä¿¡æ¯ç¼“å­˜**ï¼š

```python
async def get_user_from_cache(self, user_id: str):
    """ä»Redisç¼“å­˜è·å–ç”¨æˆ·ä¿¡æ¯"""
    if self.redis_client:
        cached_user = await self.redis_client.get(f"user:{user_id}")
        if cached_user:
            return json.loads(cached_user)
    return None
```

3. **æ›´çµæ´»çš„è·¯å¾„åŒ¹é…**ï¼š

```python
import re

def matches_excluded_pattern(self, path: str) -> bool:
    """æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼çš„è·¯å¾„åŒ¹é…"""
    patterns = [
        r"^/api/v\d+/public/.*",  # å…¬å…±API
        r"^/health/?$",           # å¥åº·æ£€æŸ¥
        r"^/metrics/?$",          # ç›‘æ§æŒ‡æ ‡
    ]
    return any(re.match(pattern, path) for pattern in patterns)
```

è¿™äº›æ”¹è¿›å¯ä»¥æå‡æ€§èƒ½å’Œçµæ´»æ€§ã€‚

>>> /bye

```

#### 3. ä¸­æ–‡å¯¹è¯ä½“éªŒ
```bash
# ä½¿ç”¨Qwen2.5è¿›è¡Œä¸­æ–‡å¯¹è¯
docker exec -it ollama-server ollama run qwen2.5:7b

>>> è¯·å¸®æˆ‘å†™ä¸€ä»½æŠ€æœ¯æ–¹æ¡ˆï¼šå¦‚ä½•åœ¨å¾®æœåŠ¡æ¶æ„ä¸­å®ç°åˆ†å¸ƒå¼é”

# å¾®æœåŠ¡æ¶æ„ä¸­çš„åˆ†å¸ƒå¼é”æŠ€æœ¯æ–¹æ¡ˆ

## 1. éœ€æ±‚åˆ†æ

### 1.1 åº”ç”¨åœºæ™¯
- **åº“å­˜æ‰£å‡**ï¼šé˜²æ­¢è¶…å–é—®é¢˜
- **è®¢å•å·ç”Ÿæˆ**ï¼šç¡®ä¿å…¨å±€å”¯ä¸€æ€§
- **å®šæ—¶ä»»åŠ¡è°ƒåº¦**ï¼šé¿å…é‡å¤æ‰§è¡Œ
- **ç¼“å­˜æ›´æ–°**ï¼šé˜²æ­¢ç¼“å­˜å‡»ç©¿

### 1.2 æŠ€æœ¯è¦æ±‚
- **äº’æ–¥æ€§**ï¼šåŒä¸€æ—¶é—´åªèƒ½æœ‰ä¸€ä¸ªå®¢æˆ·ç«¯æŒæœ‰é”
- **å®‰å…¨æ€§**ï¼šåªæœ‰é”çš„æŒæœ‰è€…æ‰èƒ½é‡Šæ”¾é”
- **æ´»æ€§**ï¼šé¿å…æ­»é”ï¼Œå…·å¤‡é”è¶…æ—¶æœºåˆ¶
- **é«˜å¯ç”¨**ï¼šé”æœåŠ¡æœ¬èº«è¦é«˜å¯ç”¨
- **æ€§èƒ½**ï¼šä½å»¶è¿Ÿï¼Œé«˜å¹¶å‘æ”¯æŒ

## 2. æŠ€æœ¯æ–¹æ¡ˆå¯¹æ¯”

### 2.1 åŸºäºRedisçš„åˆ†å¸ƒå¼é”

**ä¼˜ç‚¹ï¼š**
- æ€§èƒ½ä¼˜å¼‚ï¼Œå»¶è¿Ÿä½
- å®ç°ç®€å•ï¼Œæ”¯æŒè¿‡æœŸæ—¶é—´
- ç¤¾åŒºç”Ÿæ€å®Œå–„

**å®ç°ç¤ºä¾‹ï¼š**
```java
@Component
public class RedisDistributedLock {
    
    @Autowired
    private StringRedisTemplate redisTemplate;
    
    public boolean tryLock(String key, String requestId, int expireTime) {
        String result = redisTemplate.execute((RedisCallback<String>) connection -> {
            Jedis jedis = (Jedis) connection.getNativeConnection();
            return jedis.set(key, requestId, "NX", "PX", expireTime);
        });
        return "OK".equals(result);
    }
    
    public boolean releaseLock(String key, String requestId) {
        String luaScript = 
            "if redis.call('get', KEYS[1]) == ARGV[1] then " +
            "return redis.call('del', KEYS[1]) " +
            "else return 0 end";
        
        Object result = redisTemplate.execute((RedisCallback<Object>) connection -> {
            Jedis jedis = (Jedis) connection.getNativeConnection();
            return jedis.eval(luaScript, Collections.singletonList(key), 
                            Collections.singletonList(requestId));
        });
        return "1".equals(result.toString());
    }
}
```

### 2.2 åŸºäºZookeeperçš„åˆ†å¸ƒå¼é”

**ä¼˜ç‚¹ï¼š**

- å¼ºä¸€è‡´æ€§ä¿è¯
- æ”¯æŒå¯é‡å…¥é”
- è‡ªåŠ¨æ•…éšœæ£€æµ‹

**å®ç°æ¡†æ¶ï¼š**

```xml
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-recipes</artifactId>
    <version>5.4.0</version>
</dependency>
```

```java
@Component
public class ZookeeperDistributedLock {
    
    private CuratorFramework client;
    
    public void executeWithLock(String lockPath, Runnable task) {
        InterProcessMutex lock = new InterProcessMutex(client, lockPath);
        try {
            if (lock.acquire(30, TimeUnit.SECONDS)) {
                task.run();
            }
        } catch (Exception e) {
            log.error("è·å–é”å¤±è´¥", e);
        } finally {
            try {
                lock.release();
            } catch (Exception e) {
                log.error("é‡Šæ”¾é”å¤±è´¥", e);
            }
        }
    }
}
```

## 3. æ¨èå®ç°æ–¹æ¡ˆ

### 3.1 Redis + Redissonæ–¹æ¡ˆï¼ˆæ¨èï¼‰

```java
@Configuration
public class DistributedLockConfig {
    
    @Bean
    public RedissonClient redissonClient() {
        Config config = new Config();
        config.useSingleServer()
              .setAddress("redis://localhost:6379")
              .setPassword("your-password")
              .setConnectionPoolSize(50)
              .setConnectionMinimumIdleSize(10);
        return Redisson.create(config);
    }
}

@Service
public class BusinessService {
    
    @Autowired
    private RedissonClient redissonClient;
    
    public void processOrder(String orderId) {
        String lockKey = "order:lock:" + orderId;
        RLock lock = redissonClient.getLock(lockKey);
        
        try {
            // å°è¯•è·å–é”ï¼Œæœ€å¤šç­‰å¾…3ç§’ï¼Œé”å®š10ç§’åè‡ªåŠ¨é‡Šæ”¾
            if (lock.tryLock(3, 10, TimeUnit.SECONDS)) {
                // ä¸šåŠ¡é€»è¾‘å¤„ç†
                doProcessOrder(orderId);
            } else {
                throw new RuntimeException("è·å–é”è¶…æ—¶");
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new RuntimeException("è·å–é”è¢«ä¸­æ–­", e);
        } finally {
            if (lock.isHeldByCurrentThread()) {
                lock.unlock();
            }
        }
    }
}
```

### 3.2 æ³¨è§£å¼åˆ†å¸ƒå¼é”

```java
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface DistributedLock {
    String key();
    long waitTime() default 3000;
    long leaseTime() default 10000;
}

@Aspect
@Component
public class DistributedLockAspect {
    
    @Around("@annotation(distributedLock)")
    public Object around(ProceedingJoinPoint joinPoint, DistributedLock distributedLock) 
            throws Throwable {
        String lockKey = parseLockKey(distributedLock.key(), joinPoint);
        RLock lock = redissonClient.getLock(lockKey);
        
        try {
            if (lock.tryLock(distributedLock.waitTime(), 
                           distributedLock.leaseTime(), TimeUnit.MILLISECONDS)) {
                return joinPoint.proceed();
            } else {
                throw new RuntimeException("è·å–åˆ†å¸ƒå¼é”å¤±è´¥");
            }
        } finally {
            if (lock.isHeldByCurrentThread()) {
                lock.unlock();
            }
        }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
@DistributedLock(key = "'inventory:' + #productId", waitTime = 5000, leaseTime = 15000)
public void updateInventory(String productId, int quantity) {
    // åº“å­˜æ›´æ–°é€»è¾‘
}
```

## 4. æœ€ä½³å®è·µ

### 4.1 é”çš„ç²’åº¦æ§åˆ¶

- **ç»†ç²’åº¦é”**ï¼šæé«˜å¹¶å‘æ€§èƒ½
- **ä¸šåŠ¡éš”ç¦»**ï¼šä¸åŒä¸šåŠ¡ä½¿ç”¨ä¸åŒé”å‰ç¼€
- **é¿å…çƒ­ç‚¹**ï¼šåˆç†è®¾è®¡é”çš„Key

### 4.2 å¼‚å¸¸å¤„ç†

```java
@Component
public class SafeDistributedLock {
    
    public <T> T executeWithLock(String lockKey, Supplier<T> supplier, 
                                long waitTime, long leaseTime) {
        RLock lock = redissonClient.getLock(lockKey);
        try {
            if (lock.tryLock(waitTime, leaseTime, TimeUnit.MILLISECONDS)) {
                return supplier.get();
            } else {
                throw new BusinessException("ç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åé‡è¯•");
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new BusinessException("æ“ä½œè¢«ä¸­æ–­");
        } catch (Exception e) {
            log.error("åˆ†å¸ƒå¼é”æ‰§è¡Œå¼‚å¸¸, lockKey: {}", lockKey, e);
            throw new BusinessException("ç³»ç»Ÿå¼‚å¸¸");
        } finally {
            try {
                if (lock.isHeldByCurrentThread()) {
                    lock.unlock();
                }
            } catch (Exception e) {
                log.error("é‡Šæ”¾é”å¼‚å¸¸, lockKey: {}", lockKey, e);
            }
        }
    }
}
```

è¿™ä¸ªæ–¹æ¡ˆåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å·²ç»å¾—åˆ°éªŒè¯ï¼Œæ€§èƒ½å’Œç¨³å®šæ€§éƒ½å¾ˆå¥½ã€‚

>>> å¤ªè¯¦ç»†äº†ï¼Œè°¢è°¢ï¼è¿˜æœ‰å…¶ä»–é—®é¢˜ä¹Ÿå¯ä»¥éšæ—¶é—®æˆ‘ã€‚

>>> /bye

```

## æ–¹æ¡ˆäºŒï¼šä½¿ç”¨Text Generation WebUI

Text Generation WebUIæ˜¯å¦ä¸€ä¸ªæµè¡Œçš„æœ¬åœ°å¤§æ¨¡å‹éƒ¨ç½²æ–¹æ¡ˆï¼Œæä¾›äº†æ›´ä¸°å¯Œçš„é…ç½®é€‰é¡¹ã€‚

### ğŸ› ï¸ éƒ¨ç½²æ­¥éª¤

#### 1. åˆ›å»ºéƒ¨ç½²ç›®å½•
```bash
# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p ~/text-generation-webui
cd ~/text-generation-webui

# åˆ›å»ºæ¨¡å‹å­˜å‚¨ç›®å½•
mkdir -p models downloads
```

#### 2. åˆ›å»ºDocker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  text-generation-webui:
    image: atinoda/text-generation-webui:latest-cuda
    container_name: textgen-webui
    ports:
      - "7860:7860"
      - "5000:5000"  # APIç«¯å£
    volumes:
      - ./models:/app/text-generation-webui/models
      - ./downloads:/app/text-generation-webui/downloads
      - ./characters:/app/text-generation-webui/characters
      - ./presets:/app/text-generation-webui/presets
    environment:
      - CLI_ARGS=--api --listen --verbose
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
```

#### 3. å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹å¯åŠ¨æ—¥å¿—
docker-compose logs -f text-generation-webui

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:5000/api/v1/model
```

### ğŸ“¥ æ¨¡å‹ä¸‹è½½å’Œç®¡ç†

#### ä¸‹è½½æ¨¡å‹æ–‡ä»¶

```bash
# è¿›å…¥å®¹å™¨
docker exec -it textgen-webui bash

# ä½¿ç”¨huggingface-cliä¸‹è½½æ¨¡å‹
cd /app/text-generation-webui
python download-model.py microsoft/DialoGPT-medium

# æˆ–è€…ä¸‹è½½GGUFæ ¼å¼æ¨¡å‹
python download-model.py TheBloke/Llama-2-7B-Chat-GGUF
```

#### è‡ªåŠ¨ä¸‹è½½è„šæœ¬

```bash
#!/bin/bash
# download_models.sh

models=(
    "microsoft/DialoGPT-medium"
    "meta-llama/Llama-2-7b-chat-hf"
    "deepseek-ai/deepseek-coder-6.7b-instruct"
)

for model in "${models[@]}"; do
    echo "Downloading $model..."
    docker exec textgen-webui python download-model.py "$model"
done

echo "All models downloaded!"
```

### ğŸ”§ APIè°ƒç”¨ç¤ºä¾‹

#### Pythonå®¢æˆ·ç«¯

```python
#!/usr/bin/env python3
# chat_client.py

import requests
import json
import sys

class TextGenClient:
    def __init__(self, base_url="http://localhost:5000"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def load_model(self, model_name):
        """åŠ è½½æŒ‡å®šæ¨¡å‹"""
        payload = {
            "model_name": model_name
        }
        response = self.session.post(
            f"{self.base_url}/api/v1/model",
            json=payload
        )
        return response.json()
    
    def chat(self, message, history=None):
        """å‘é€èŠå¤©æ¶ˆæ¯"""
        if history is None:
            history = []
        
        payload = {
            "user_input": message,
            "history": history,
            "mode": "chat",
            "character": "Assistant"
        }
        
        response = self.session.post(
            f"{self.base_url}/api/v1/chat",
            json=payload
        )
        
        if response.status_code == 200:
            data = response.json()
            return data.get("results", [{}])[0].get("history", {})
        else:
            return {"error": f"Request failed: {response.status_code}"}
    
    def generate(self, prompt, max_tokens=512):
        """ç”Ÿæˆæ–‡æœ¬"""
        payload = {
            "prompt": prompt,
            "max_new_tokens": max_tokens,
            "temperature": 0.7,
            "top_p": 0.9,
            "do_sample": True
        }
        
        response = self.session.post(
            f"{self.base_url}/api/v1/generate",
            json=payload
        )
        
        if response.status_code == 200:
            return response.json()["results"][0]["text"]
        else:
            return f"Error: {response.status_code}"

def main():
    client = TextGenClient()
    
    print("ğŸ¤– æœ¬åœ°AIåŠ©æ‰‹å·²å¯åŠ¨!")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
    print("è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("-" * 50)
    
    history = []
    
    while True:
        try:
            user_input = input("\nğŸ§‘ æ‚¨: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'bye']:
                print("ğŸ‘‹ å†è§!")
                break
            
            if user_input.lower() == 'clear':
                history = []
                print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            
            if not user_input:
                continue
            
            print("ğŸ¤– AI: ", end="", flush=True)
            
            # å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤
            result = client.chat(user_input, history)
            
            if "error" in result:
                print(f"âŒ é”™è¯¯: {result['error']}")
            else:
                # æ›´æ–°å¯¹è¯å†å²
                history = result.get("history", [])
                
                # æ˜¾ç¤ºAIå›å¤
                if history:
                    ai_response = history[-1][1] if len(history[-1]) > 1 else "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ã€‚"
                    print(ai_response)
                else:
                    print("æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚")
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
```

#### ä½¿ç”¨å®¢æˆ·ç«¯

```bash
# å®‰è£…ä¾èµ–
pip install requests

# è¿è¡ŒèŠå¤©å®¢æˆ·ç«¯
python chat_client.py

# ç¤ºä¾‹å¯¹è¯
ğŸ¤– æœ¬åœ°AIåŠ©æ‰‹å·²å¯åŠ¨!
è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº
è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²
--------------------------------------------------

ğŸ§‘ æ‚¨: ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±

ğŸ¤– AI: ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªæœ¬åœ°éƒ¨ç½²çš„AIåŠ©æ‰‹ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š

1. å›ç­”å„ç§é—®é¢˜å’Œæä¾›ä¿¡æ¯
2. ååŠ©ç¼–ç¨‹å’Œä»£ç è°ƒè¯•
3. å¸®åŠ©å†™ä½œå’Œåˆ›ä½œ
4. è¿›è¡Œç¿»è¯‘å’Œè¯­è¨€å­¦ä¹ 
5. åˆ†æå’Œæ€»ç»“æ–‡æœ¬å†…å®¹

æˆ‘è¿è¡Œåœ¨æ‚¨çš„æœ¬åœ°ç¯å¢ƒä¸­ï¼Œè¿™æ„å‘³ç€ï¼š
- æ‚¨çš„æ•°æ®å®Œå…¨ç§å¯†ï¼Œä¸ä¼šä¸Šä¼ åˆ°äº‘ç«¯
- å“åº”é€Ÿåº¦å–å†³äºæ‚¨çš„ç¡¬ä»¶é…ç½®
- å¯ä»¥ç¦»çº¿ä½¿ç”¨ï¼Œæ— éœ€ç½‘ç»œè¿æ¥

æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ

ğŸ§‘ æ‚¨: å†™ä¸€ä¸ªPythonè£…é¥°å™¨ç”¨äºè®°å½•å‡½æ•°æ‰§è¡Œæ—¶é—´

ğŸ¤– AI: è¿™é‡Œæ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„Pythonè£…é¥°å™¨ï¼Œç”¨äºè®°å½•å‡½æ•°æ‰§è¡Œæ—¶é—´ï¼š

```python
import time
import functools
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def timing_decorator(func=None, *, unit='seconds', log_level='INFO'):
    """
    è®°å½•å‡½æ•°æ‰§è¡Œæ—¶é—´çš„è£…é¥°å™¨
    
    Args:
        func: è¢«è£…é¥°çš„å‡½æ•°
        unit: æ—¶é—´å•ä½ ('seconds', 'milliseconds', 'microseconds')
        log_level: æ—¥å¿—çº§åˆ« ('DEBUG', 'INFO', 'WARNING', 'ERROR')
    """
    def decorator(f):
        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            start_time = time.perf_counter()
            
            try:
                result = f(*args, **kwargs)
                success = True
            except Exception as e:
                result = e
                success = False
            
            end_time = time.perf_counter()
            execution_time = end_time - start_time
            
            # è½¬æ¢æ—¶é—´å•ä½
            if unit == 'milliseconds':
                execution_time *= 1000
                time_unit = 'ms'
            elif unit == 'microseconds':
                execution_time *= 1000000
                time_unit = 'Î¼s'
            else:
                time_unit = 's'
            
            # è®°å½•æ—¥å¿—
            status = "æˆåŠŸ" if success else "å¤±è´¥"
            log_message = f"å‡½æ•° {f.__name__} æ‰§è¡Œ{status} - è€—æ—¶: {execution_time:.4f} {time_unit}"
            
            # æ ¹æ®æŒ‡å®šçš„æ—¥å¿—çº§åˆ«è®°å½•
            log_func = getattr(logger, log_level.lower(), logger.info)
            log_func(log_message)
            
            if not success:
                raise result
            
            return result
        
        return wrapper
    
    # æ”¯æŒå¸¦å‚æ•°å’Œä¸å¸¦å‚æ•°çš„ä½¿ç”¨æ–¹å¼
    if func is None:
        return decorator
    else:
        return decorator(func)

# ä½¿ç”¨ç¤ºä¾‹

# 1. ç®€å•ä½¿ç”¨
@timing_decorator
def simple_function():
    time.sleep(1)
    return "Hello World"

# 2. æŒ‡å®šæ—¶é—´å•ä½
@timing_decorator(unit='milliseconds')
def compute_heavy_task():
    return sum(i**2 for i in range(1000000))

# 3. æŒ‡å®šæ—¥å¿—çº§åˆ«
@timing_decorator(unit='microseconds', log_level='DEBUG')
def quick_function():
    return [i for i in range(100)]

# 4. ç±»æ–¹æ³•è£…é¥°
class Calculator:
    @timing_decorator(unit='milliseconds')
    def fibonacci(self, n):
        if n <= 1:
            return n
        return self.fibonacci(n-1) + self.fibonacci(n-2)

# 5. é«˜çº§ç‰ˆæœ¬ï¼šå¯é…ç½®çš„è£…é¥°å™¨
def advanced_timing(
    enabled=True, 
    unit='seconds', 
    log_level='INFO',
    custom_message=None,
    store_results=False
):
    """
    é«˜çº§è®¡æ—¶è£…é¥°å™¨
    
    Args:
        enabled: æ˜¯å¦å¯ç”¨è®¡æ—¶
        unit: æ—¶é—´å•ä½
        log_level: æ—¥å¿—çº§åˆ«
        custom_message: è‡ªå®šä¹‰æ—¥å¿—æ¶ˆæ¯
        store_results: æ˜¯å¦å­˜å‚¨æ‰§è¡Œç»“æœåˆ°å…¨å±€å˜é‡
    """
    execution_results = []
    
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            if not enabled:
                return func(*args, **kwargs)
            
            start_time = time.perf_counter()
            
            try:
                result = func(*args, **kwargs)
                success = True
                error = None
            except Exception as e:
                result = None
                success = False
                error = str(e)
            
            end_time = time.perf_counter()
            execution_time = end_time - start_time
            
            # è½¬æ¢æ—¶é—´å•ä½
            time_conversions = {
                'milliseconds': (1000, 'ms'),
                'microseconds': (1000000, 'Î¼s'),
                'seconds': (1, 's')
            }
            
            multiplier, time_unit = time_conversions.get(unit, (1, 's'))
            converted_time = execution_time * multiplier
            
            # æ„å»ºæ—¥å¿—æ¶ˆæ¯
            if custom_message:
                log_message = custom_message.format(
                    func_name=func.__name__,
                    time=converted_time,
                    unit=time_unit,
                    status="æˆåŠŸ" if success else "å¤±è´¥"
                )
            else:
                status = "æˆåŠŸ" if success else f"å¤±è´¥({error})"
                log_message = f"[{func.__name__}] æ‰§è¡Œ{status} - è€—æ—¶: {converted_time:.4f} {time_unit}"
            
            # è®°å½•æ—¥å¿—
            log_func = getattr(logger, log_level.lower(), logger.info)
            log_func(log_message)
            
            # å­˜å‚¨ç»“æœ
            if store_results:
                execution_results.append({
                    'function': func.__name__,
                    'execution_time': converted_time,
                    'unit': time_unit,
                    'success': success,
                    'error': error,
                    'timestamp': time.time()
                })
            
            if not success:
                raise Exception(error)
            
            return result
        
        # æ·»åŠ è·å–æ‰§è¡Œç»“æœçš„æ–¹æ³•
        wrapper.get_execution_results = lambda: execution_results.copy()
        wrapper.clear_execution_results = lambda: execution_results.clear()
        
        return wrapper
    
    return decorator

# ä½¿ç”¨é«˜çº§ç‰ˆæœ¬ç¤ºä¾‹
@advanced_timing(
    unit='milliseconds',
    custom_message="âš¡ {func_name} å®Œæˆ - ç”¨æ—¶ {time:.2f}{unit} [{status}]",
    store_results=True
)
def data_processing_task():
    # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
    time.sleep(0.5)
    return {"processed": 1000, "errors": 0}

if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•è®¡æ—¶è£…é¥°å™¨...")
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    simple_function()
    compute_heavy_task()
    
    # æµ‹è¯•ç±»æ–¹æ³•
    calc = Calculator()
    result = calc.fibonacci(10)
    print(f"Fibonacci(10) = {result}")
    
    # æµ‹è¯•é«˜çº§ç‰ˆæœ¬
    data_processing_task()
    
    # æŸ¥çœ‹æ‰§è¡Œç»“æœ
    results = data_processing_task.get_execution_results()
    print(f"ğŸ“Š æ‰§è¡Œå†å²: {results}")
```

è¿™ä¸ªè£…é¥°å™¨æä¾›äº†å¤šç§é…ç½®é€‰é¡¹ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬ä½¿ç”¨ã€‚

ğŸ§‘ æ‚¨: clear

âœ… å¯¹è¯å†å²å·²æ¸…ç©º

ğŸ§‘ æ‚¨: quit

ğŸ‘‹ å†è§!

```

## æ–¹æ¡ˆä¸‰ï¼šä½¿ç”¨LocalAI

LocalAIæ˜¯ä¸€ä¸ªå®Œå…¨å…¼å®¹OpenAI APIçš„æœ¬åœ°æ¨ç†æœåŠ¡ï¼Œæ”¯æŒå¤šç§æ¨¡å‹æ ¼å¼ã€‚

### ğŸš€ å¿«é€Ÿéƒ¨ç½²

#### 1. åˆ›å»ºé…ç½®æ–‡ä»¶
```bash
# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p ~/localai/{models,config}
cd ~/localai
```

#### 2. åˆ›å»ºDocker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.6'

services:
  localai:
    image: quay.io/go-skynet/local-ai:v2.17.1-cublas-cuda12
    container_name: localai
    ports:
      - "8080:8080"
    environment:
      - DEBUG=true
      - MODELS_PATH=/app/models
      - CONTEXT_SIZE=1024
      - THREADS=4
    volumes:
      - ./models:/app/models:cached
      - ./config:/app/config:cached
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
```

#### 3. ä¸‹è½½æ¨¡å‹é…ç½®

```bash
# ä¸‹è½½é¢„é…ç½®çš„æ¨¡å‹
curl -O https://raw.githubusercontent.com/go-skynet/LocalAI/master/examples/configurations/llama.yaml
mv llama.yaml config/

# ä¸‹è½½æ¨¡å‹æ–‡ä»¶
cd models
wget https://huggingface.co/microsoft/DialoGPT-medium/resolve/main/pytorch_model.bin
```

#### 4. å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨LocalAIæœåŠ¡
docker-compose up -d

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8080/readiness

# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://localhost:8080/v1/models
```

### ğŸ’¬ OpenAIå…¼å®¹å®¢æˆ·ç«¯

#### Pythonå®¢æˆ·ç«¯

```python
#!/usr/bin/env python3
# localai_chat.py

import openai
import json
import sys
from datetime import datetime

class LocalAIChat:
    def __init__(self, base_url="http://localhost:8080/v1", api_key="not-needed"):
        self.client = openai.OpenAI(
            base_url=base_url,
            api_key=api_key
        )
        self.conversation_history = []
    
    def list_models(self):
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        try:
            models = self.client.models.list()
            return [model.id for model in models.data]
        except Exception as e:
            print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def chat(self, message, model="llama", system_prompt=None):
        """å‘é€èŠå¤©æ¶ˆæ¯"""
        try:
            # æ„å»ºæ¶ˆæ¯å†å²
            messages = []
            
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            
            # æ·»åŠ å†å²å¯¹è¯
            messages.extend(self.conversation_history)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": message})
            
            # è°ƒç”¨API
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=512,
                temperature=0.7,
                stream=False
            )
            
            assistant_message = response.choices[0].message.content
            
            # æ›´æ–°å¯¹è¯å†å²
            self.conversation_history.append({"role": "user", "content": message})
            self.conversation_history.append({"role": "assistant", "content": assistant_message})
            
            # ä¿æŒå†å²é•¿åº¦åœ¨åˆç†èŒƒå›´å†…
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[-20:]
            
            return assistant_message
            
        except Exception as e:
            return f"âŒ è¯·æ±‚å¤±è´¥: {e}"
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
    
    def save_conversation(self, filename=None):
        """ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"conversation_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
            return f"âœ… å¯¹è¯å·²ä¿å­˜åˆ° {filename}"
        except Exception as e:
            return f"âŒ ä¿å­˜å¤±è´¥: {e}"
    
    def load_conversation(self, filename):
        """ä»æ–‡ä»¶åŠ è½½å¯¹è¯"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.conversation_history = json.load(f)
            return f"âœ… å¯¹è¯å·²ä» {filename} åŠ è½½"
        except Exception as e:
            return f"âŒ åŠ è½½å¤±è´¥: {e}"

def print_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    help_text = """
ğŸ¤– LocalAI èŠå¤©å®¢æˆ·ç«¯ - å¯ç”¨å‘½ä»¤:

åŸºæœ¬å‘½ä»¤:
  help          - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  models        - åˆ—å‡ºå¯ç”¨æ¨¡å‹
  clear         - æ¸…ç©ºå¯¹è¯å†å²
  quit/exit     - é€€å‡ºç¨‹åº

æ–‡ä»¶æ“ä½œ:
  save [filename]    - ä¿å­˜å¯¹è¯ (å¯é€‰æŒ‡å®šæ–‡ä»¶å)
  load <filename>    - åŠ è½½ä¹‹å‰çš„å¯¹è¯

æ¨¡å‹åˆ‡æ¢:
  use <model_name>   - åˆ‡æ¢ä½¿ç”¨çš„æ¨¡å‹

å…¶ä»–:
  /system <prompt>   - è®¾ç½®ç³»ç»Ÿæç¤ºè¯
  /status           - æ˜¾ç¤ºå½“å‰çŠ¶æ€
    """
    print(help_text)

def main():
    chat = LocalAIChat()
    current_model = "llama"
    system_prompt = None
    
    print("ğŸš€ LocalAI èŠå¤©å®¢æˆ·ç«¯å·²å¯åŠ¨!")
    print("è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
    print("-" * 60)
    
    # æ£€æŸ¥å¯ç”¨æ¨¡å‹
    print("ğŸ” æ£€æŸ¥å¯ç”¨æ¨¡å‹...")
    available_models = chat.list_models()
    if available_models:
        print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {', '.join(available_models)}")
        current_model = available_models[0]
        print(f"ğŸ¯ å½“å‰ä½¿ç”¨æ¨¡å‹: {current_model}")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥LocalAIæœåŠ¡çŠ¶æ€")
    
    print("-" * 60)
    
    while True:
        try:
            user_input = input(f"\nğŸ§‘ æ‚¨ [{current_model}]: ").strip()
            
            if not user_input:
                continue
            
            # å¤„ç†å‘½ä»¤
            if user_input.lower() in ['quit', 'exit', 'bye']:
                print("ğŸ‘‹ å†è§!")
                break
            
            elif user_input.lower() == 'help':
                print_help()
                continue
            
            elif user_input.lower() == 'models':
                models = chat.list_models()
                if models:
                    print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {', '.join(models)}")
                else:
                    print("âŒ æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨")
                continue
            
            elif user_input.lower() == 'clear':
                chat.clear_history()
                print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            
            elif user_input.lower().startswith('save'):
                parts = user_input.split(' ', 1)
                filename = parts[1] if len(parts) > 1 else None
                result = chat.save_conversation(filename)
                print(result)
                continue
            
            elif user_input.lower().startswith('load'):
                parts = user_input.split(' ', 1)
                if len(parts) > 1:
                    result = chat.load_conversation(parts[1])
                    print(result)
                else:
                    print("âŒ è¯·æŒ‡å®šè¦åŠ è½½çš„æ–‡ä»¶å")
                continue
            
            elif user_input.lower().startswith('use'):
                parts = user_input.split(' ', 1)
                if len(parts) > 1:
                    new_model = parts[1]
                    if new_model in available_models:
                        current_model = new_model
                        print(f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {current_model}")
                    else:
                        print(f"âŒ æ¨¡å‹ '{new_model}' ä¸å¯ç”¨")
                else:
                    print("âŒ è¯·æŒ‡å®šæ¨¡å‹åç§°")
                continue
            
            elif user_input.startswith('/system'):
                system_prompt = user_input[7:].strip()
                print(f"âœ… ç³»ç»Ÿæç¤ºè¯å·²è®¾ç½®: {system_prompt}")
                continue
            
            elif user_input == '/status':
                print(f"ğŸ“Š å½“å‰çŠ¶æ€:")
                print(f"   æ¨¡å‹: {current_model}")
                print(f"   ç³»ç»Ÿæç¤ºè¯: {system_prompt or 'æœªè®¾ç½®'}")
                print(f"   å¯¹è¯å†å²é•¿åº¦: {len(chat.conversation_history)}")
                continue
            
            # å‘é€èŠå¤©æ¶ˆæ¯
            print("ğŸ¤– AI: ", end="", flush=True)
            
            response = chat.chat(user_input, current_model, system_prompt)
            print(response)
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
```

#### ä½¿ç”¨ç¤ºä¾‹

```bash
# å®‰è£…ä¾èµ–
pip install openai

# è¿è¡ŒèŠå¤©å®¢æˆ·ç«¯
python localai_chat.py

# ç¤ºä¾‹äº¤äº’
ğŸš€ LocalAI èŠå¤©å®¢æˆ·ç«¯å·²å¯åŠ¨!
è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤
------------------------------------------------------------
ğŸ” æ£€æŸ¥å¯ç”¨æ¨¡å‹...
ğŸ“‹ å¯ç”¨æ¨¡å‹: llama, gpt-3.5-turbo, text-davinci-003
ğŸ¯ å½“å‰ä½¿ç”¨æ¨¡å‹: llama
------------------------------------------------------------

ğŸ§‘ æ‚¨ [llama]: /system ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„æ–¹å¼å›ç­”é—®é¢˜

âœ… ç³»ç»Ÿæç¤ºè¯å·²è®¾ç½®: ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„æ–¹å¼å›ç­”é—®é¢˜

ğŸ§‘ æ‚¨ [llama]: å¦‚ä½•åœ¨Pythonä¸­åˆ›å»ºä¸€ä¸ªå•ä¾‹æ¨¡å¼ï¼Ÿ

ğŸ¤– AI: è¿™é‡Œæ˜¯å‡ ç§Pythonå•ä¾‹æ¨¡å¼çš„å®ç°æ–¹æ³•ï¼š

## 1. ä½¿ç”¨__new__æ–¹æ³•ï¼ˆæ¨èï¼‰
```python
class Singleton:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

# ä½¿ç”¨
s1 = Singleton()
s2 = Singleton()
print(s1 is s2)  # True
```

## 2. è£…é¥°å™¨æ–¹å¼

```python
def singleton(cls):
    instances = {}
    def get_instance(*args, **kwargs):
        if cls not in instances:
            instances[cls] = cls(*args, **kwargs)
        return instances[cls]
    return get_instance

@singleton
class MyClass:
    pass
```

## 3. å…ƒç±»æ–¹å¼

```python
class SingletonMeta(type):
    _instances = {}
    
    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super().__call__(*args, **kwargs)
        return cls._instances[cls]

class Singleton(metaclass=SingletonMeta):
    pass
```

æ¨èä½¿ç”¨ç¬¬ä¸€ç§æ–¹æ³•ï¼Œç®€å•æ˜“æ‡‚ä¸”çº¿ç¨‹å®‰å…¨ã€‚

ğŸ§‘ æ‚¨ [llama]: save python_singleton_chat

âœ… å¯¹è¯å·²ä¿å­˜åˆ° python_singleton_chat

ğŸ§‘ æ‚¨ [llama]: quit

ğŸ‘‹ å†è§!

```

## æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### ğŸ“Š èµ„æºç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# monitor_resources.sh

echo "ğŸ” æœ¬åœ°AIæ¨¡å‹èµ„æºç›‘æ§"
echo "================================"

while true; do
    clear
    echo "ğŸ“… æ—¶é—´: $(date)"
    echo "================================"
    
    # GPUç›‘æ§
    if command -v nvidia-smi &> /dev/null; then
        echo "ğŸ–¥ï¸  GPUä½¿ç”¨æƒ…å†µ:"
        nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv,noheader,nounits | \
        while IFS=, read -r index name util mem_used mem_total temp; do
            printf "GPU%s: %s | ä½¿ç”¨ç‡: %s%% | æ˜¾å­˜: %s/%sMB | æ¸©åº¦: %sÂ°C\n" \
                   "$index" "$name" "$util" "$mem_used" "$mem_total" "$temp"
        done
        echo ""
    fi
    
    # CPUå’Œå†…å­˜ç›‘æ§
    echo "ğŸ’» CPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µ:"
    top -bn1 | head -20 | grep -E "(Cpu|Mem|swap)" | \
    sed 's/%Cpu(s):/CPU:/' | sed 's/KiB Mem :/å†…å­˜:/' | sed 's/KiB Swap:/äº¤æ¢:'
    echo ""
    
    # Dockerå®¹å™¨ç›‘æ§
    echo "ğŸ³ Dockerå®¹å™¨çŠ¶æ€:"
    docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}" | \
    grep -E "(ollama|textgen|localai)"
    echo ""
    
    echo "æŒ‰ Ctrl+C é€€å‡ºç›‘æ§"
    sleep 5
done
```

### ğŸ¯ æ€§èƒ½è°ƒä¼˜å»ºè®®

#### 1. å†…å­˜ä¼˜åŒ–

```bash
# å¢åŠ swapç©ºé—´ï¼ˆå¦‚æœå†…å­˜ä¸è¶³ï¼‰
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# æ°¸ä¹…å¯ç”¨swap
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

#### 2. Dockerä¼˜åŒ–

```bash
# æ¸…ç†æœªä½¿ç”¨çš„Dockerèµ„æº
docker system prune -a -f

# é™åˆ¶å®¹å™¨èµ„æºä½¿ç”¨
docker run -d \
  --name ollama-server \
  --memory="16g" \
  --cpus="4.0" \
  --gpus all \
  -p 11434:11434 \
  -v ollama-data:/root/.ollama \
  ollama/ollama
```

#### 3. æ¨¡å‹é€‰æ‹©ç­–ç•¥

```bash
# æ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹
hardware_check() {
    total_mem=$(free -g | awk '/^Mem:/{print $2}')
    gpu_mem=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    
    echo "ğŸ’» ç³»ç»Ÿå†…å­˜: ${total_mem}GB"
    echo "ğŸ–¥ï¸  GPUæ˜¾å­˜: ${gpu_mem}MB"
    
    if [ "$total_mem" -ge 32 ] && [ "$gpu_mem" -ge 12000 ]; then
        echo "âœ… æ¨èæ¨¡å‹: llama3.1:8b, deepseek-coder:6.7b"
    elif [ "$total_mem" -ge 16 ] && [ "$gpu_mem" -ge 8000 ]; then
        echo "âœ… æ¨èæ¨¡å‹: llama3.1:8b-q4, qwen2.5:7b-q4"
    else
        echo "âœ… æ¨èæ¨¡å‹: llama3.1:8b-q8, phi3:mini"
    fi
}

hardware_check
```

## æ•…éšœæ’é™¤

### ğŸ”§ å¸¸è§é—®é¢˜è§£å†³

#### 1. å†…å­˜ä¸è¶³

```bash
# ç—‡çŠ¶ï¼šå®¹å™¨é¢‘ç¹é‡å¯æˆ–å¡æ­»
# è§£å†³æ–¹æ¡ˆï¼š
# 1. ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬æ¨¡å‹
docker exec ollama-server ollama pull llama3.1:8b-q4_0

# 2. å¢åŠ swapç©ºé—´
sudo dd if=/dev/zero of=/swapfile bs=1M count=8192
sudo mkswap /swapfile
sudo swapon /swapfile

# 3. é™åˆ¶æ¨¡å‹å¹¶å‘æ•°
echo "OLLAMA_NUM_PARALLEL=1" >> ~/.bashrc
```

#### 2. GPUä¸è¢«è¯†åˆ«

```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# å®‰è£…nvidia-container-toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

#### 3. æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# ä½¿ç”¨é•œåƒç«™ç‚¹
export HF_ENDPOINT=https://hf-mirror.com

# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
wget -c https://hf-mirror.com/microsoft/DialoGPT-medium/resolve/main/pytorch_model.bin

# ä½¿ç”¨ä»£ç†ä¸‹è½½
docker run --rm -e HTTP_PROXY=http://your-proxy:port \
  ollama/ollama pull llama3.1:8b
```

### ğŸ› ï¸ è¯Šæ–­è„šæœ¬

```bash
#!/bin/bash
# diagnose.sh

echo "ğŸ” æœ¬åœ°AIéƒ¨ç½²è¯Šæ–­å·¥å…·"
echo "=========================="

# æ£€æŸ¥ç³»ç»Ÿèµ„æº
echo "1. ç³»ç»Ÿèµ„æºæ£€æŸ¥"
echo "å†…å­˜: $(free -h | grep Mem | awk '{print $3"/"$2}')"
echo "ç£ç›˜: $(df -h / | tail -1 | awk '{print $3"/"$2" ("$5" used)"}')"

# æ£€æŸ¥Docker
echo -e "\n2. Dockerç¯å¢ƒæ£€æŸ¥"
if command -v docker &> /dev/null; then
    echo "âœ… Dockerå·²å®‰è£…: $(docker --version)"
    if docker info &> /dev/null; then
        echo "âœ… DockeræœåŠ¡æ­£å¸¸"
    else
        echo "âŒ DockeræœåŠ¡å¼‚å¸¸"
    fi
else
    echo "âŒ Dockeræœªå®‰è£…"
fi

# æ£€æŸ¥GPU
echo -e "\n3. GPUç¯å¢ƒæ£€æŸ¥"
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… NVIDIAé©±åŠ¨å·²å®‰è£…"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
    
    if docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi &> /dev/null; then
        echo "âœ… Docker GPUæ”¯æŒæ­£å¸¸"
    else
        echo "âŒ Docker GPUæ”¯æŒå¼‚å¸¸"
    fi
else
    echo "âš ï¸  æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–é©±åŠ¨"
fi

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
echo -e "\n4. AIå®¹å™¨çŠ¶æ€æ£€æŸ¥"
containers=("ollama-server" "textgen-webui" "localai")
for container in "${containers[@]}"; do
    if docker ps | grep -q "$container"; then
        echo "âœ… $container è¿è¡Œä¸­"
    elif docker ps -a | grep -q "$container"; then
        echo "âš ï¸  $container å·²åœæ­¢"
    else
        echo "âŒ $container æœªæ‰¾åˆ°"
    fi
done

# æ£€æŸ¥ç«¯å£å ç”¨
echo -e "\n5. ç«¯å£å ç”¨æ£€æŸ¥"
ports=(11434 7860 5000 8080)
for port in "${ports[@]}"; do
    if ss -tlnp | grep -q ":$port "; then
        echo "âœ… ç«¯å£ $port è¢«å ç”¨"
    else
        echo "âŒ ç«¯å£ $port æœªè¢«å ç”¨"
    fi
done

echo -e "\nè¯Šæ–­å®Œæˆï¼"
```

## æ€»ç»“ä¸å±•æœ›

é€šè¿‡æœ¬æ–‡çš„è¯¦ç»†ä»‹ç»ï¼Œæˆ‘ä»¬å·²ç»æŒæ¡äº†ä½¿ç”¨Dockeréƒ¨ç½²æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹çš„å¤šç§æ–¹æ¡ˆã€‚æ¯ç§æ–¹æ¡ˆéƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼š

### ğŸ¯ æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | é€‚ç”¨åœºæ™¯ | æ¨èæŒ‡æ•° |
|------|------|----------|----------|
| **Ollama** | ç®€å•æ˜“ç”¨ï¼Œæ¨¡å‹ç®¡ç†æ–¹ä¾¿ | ä¸ªäººä½¿ç”¨ï¼Œå¿«é€Ÿä½“éªŒ | â­â­â­â­â­ |
| **Text Generation WebUI** | åŠŸèƒ½ä¸°å¯Œï¼Œé…ç½®çµæ´» | ç ”ç©¶å®éªŒï¼Œé«˜çº§ç”¨æˆ· | â­â­â­â­ |
| **LocalAI** | OpenAIå…¼å®¹ï¼Œç”Ÿæ€å®Œå–„ | ä¼ä¸šé›†æˆï¼ŒAPIæœåŠ¡ | â­â­â­â­ |

### ğŸš€ æœ€ä½³å®è·µå»ºè®®

1. **æ–°æ‰‹å…¥é—¨**ï¼šæ¨èä»Ollamaå¼€å§‹ï¼Œç®€å•ç›´æ¥
2. **å¼€å‘é›†æˆ**ï¼šé€‰æ‹©LocalAIï¼Œå…¼å®¹æ€§æœ€å¥½
3. **ç ”ç©¶å®éªŒ**ï¼šä½¿ç”¨Text Generation WebUIï¼Œé…ç½®æœ€çµæ´»
4. **ç”Ÿäº§éƒ¨ç½²**ï¼šç»“åˆå…·ä½“éœ€æ±‚é€‰æ‹©ï¼Œæ³¨é‡ç¨³å®šæ€§å’Œæ€§èƒ½

### ğŸ’¡ æœªæ¥å‘å±•æ–¹å‘

- **æ¨¡å‹æ•ˆç‡æå‡**ï¼šæ›´å°çš„æ¨¡å‹ï¼Œæ›´å¼ºçš„èƒ½åŠ›
- **ç¡¬ä»¶ä¼˜åŒ–**ï¼šæ›´å¥½çš„GPUåˆ©ç”¨ç‡å’Œå†…å­˜ç®¡ç†
- **éƒ¨ç½²ç®€åŒ–**ï¼šä¸€é”®éƒ¨ç½²ï¼Œè‡ªåŠ¨åŒ–é…ç½®
- **å¤šæ¨¡æ€æ”¯æŒ**ï¼šå›¾åƒã€è¯­éŸ³ç­‰å¤šæ¨¡æ€èƒ½åŠ›é›†æˆ

æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²ä¸ä»…ä¿æŠ¤äº†æ•°æ®éšç§ï¼Œè¿˜ä¸ºæˆ‘ä»¬æä¾›äº†å®Œå…¨å¯æ§çš„AIä½“éªŒã€‚éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œç›¸ä¿¡æœ¬åœ°AIåŠ©æ‰‹ä¼šå˜å¾—æ›´åŠ æ™ºèƒ½å’Œæ˜“ç”¨ï¼Œæˆä¸ºæˆ‘ä»¬æ—¥å¸¸å·¥ä½œå’Œå­¦ä¹ çš„å¾—åŠ›ä¼™ä¼´ã€‚

---

## ç›¸å…³èµ„æº

### ğŸ“š å®˜æ–¹æ–‡æ¡£

- [Ollamaå®˜æ–¹æ–‡æ¡£](https://ollama.ai/docs)
- [Text Generation WebUIé¡¹ç›®](https://github.com/oobabooga/text-generation-webui)
- [LocalAIé¡¹ç›®æ–‡æ¡£](https://localai.io/)

### ğŸ”— æœ‰ç”¨é“¾æ¥

- [Hugging Faceæ¨¡å‹åº“](https://huggingface.co/models)
- [Dockerå®˜æ–¹æ–‡æ¡£](https://docs.docker.com/)
- [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-container-toolkit)

### ğŸ› ï¸ å·¥å…·æ¨è

- [GPUç›‘æ§å·¥å…·](https://github.com/wookayin/gpustat)
- [Dockerèµ„æºç›‘æ§](https://github.com/bcicen/ctop)
- [æ¨¡å‹è½¬æ¢å·¥å…·](https://github.com/ggerganov/llama.cpp)
